# -*- coding: utf-8 -*-
"""quantum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vEz9Ge_ZRnJwneJbeQpzdV70w6V2S4b3
"""

def get_reward(X_n):
    A = 4000
    reward = []

    # 繁忙时期 qubit较少 a>b>c>d

    # route a, 1 hop, 2 edges
    # 单跳单qubit单次纠缠成功率
    pe_a_1 = 1.5e-4
    pe_a_2 = 1.5e-4
    # 单跳单qubit单slot(A次)纠缠成功率
    p_a_1 = 1 - (1 - pe_a_1)**A
    p_a_2 = 1 - (1 - pe_a_2)**A
#     # 单跳qubit数量
#     n_a_1 = 4
#     n_a_2 = 4
    # 单跳成功率
    for j in range(len(X_n)):
#         P_a_1 = 1 - (1 - p_a_1)**n_a_1
#         P_a_2 = 1 - (1 - p_a_2)**n_a_2
        P_a_1 = 1 - (1 - p_a_1)**X_n[j][0]
        P_a_2 = 1 - (1 - p_a_2)**X_n[j][1]
        # route a 成功率
        P_a = P_a_1 * P_a_2
        reward.append(P_a)
    return reward

from env import *
from neuralucb import *
import matplotlib.pyplot as plt
import torch
from tqdm import trange
import random



X_n = []
qubit = 8
for i in range(qubit+1):
    new_d = np.array([i,qubit-i])
    X_n.append(new_d)
X_n = np.array(X_n)

# for i in range(3):
#     front = np.zeros((4 * i))
#     back = np.zeros((4 * (2 - i)))
#     new_d = np.concatenate((front, x, back), axis=0)
#     X_n.append(new_d)
# X_n = np.array(X_n)

neuralucb = NeuralUCB(2, len(X_n), beta=1, lamb=1)
NeuralUCBregret_list = []
NeuralUCBregret = 0
# NeuralTSregret = [0]

context = X_n
reward = get_reward(X_n)
print(X_n)
print(reward)
for i in trange(10000):
#     context, reward = env.step()
    arm = neuralucb.take_action(context)
#     X_t = np.random.binomial(1, reward[arm])
    X_t = np.random.choice([0,1],p=[1-reward[arm], reward[arm]])
    print('reward[arm]:',reward[arm])
#     X_t = random.choices([0, 1], weights=[1-reward[arm], reward[arm]], k=1)[0]
#     NeuralTSregret += [NeuralTSregret[-1] + max(reward) - reward[arm]]
    NeuralUCBregret = NeuralUCBregret +  max(reward) - reward[arm]
    NeuralUCBregret_list.append(NeuralUCBregret)
#     neuralucb.update(context, arm, reward[arm])
    neuralucb.update(context, arm, X_t)


# plt.plot(NeuralUCBregret, label='NeuralUCB')
# print('neuralucb:', NeuralUCBregret[-1])

import matplotlib.pyplot as plt
import numpy as np

t = np.arange(0,2000,1)

# plt.title('g=3,d=2,k=100,N=100')
plt.plot(t, NeuralUCBregret_list[0:2000], color='red', label = 'Neural_UCB',linestyle="--",linewidth=2)
# plt.plot(t, regret_linucb_list, color='green', label = 'Linucb',linestyle="--",linewidth=2)
# plt.plot(t, regret_glinucb_list, color='blue', label = 'GLinucb',linestyle="--",linewidth=2)
plt.legend()
plt.grid()
plt.xlabel('Frame Number')
plt.ylabel('Cumulative Regret')
# plt.savefig('g=3,d=2,k=100,N=100.png', format='png')
plt.show()

print(NeuralUCBregret_list)

"""# LinUCB"""

linucb = LinUCB(2, len(X_n), beta=0.2, lamb=1)
# LinUCBregret = [0]

LinUCBregret = 0
LinUCBregret_list = []
LinUCBreward_list = []
LinUCBreward = 0

for i in range(10000):
#     context, reward = env.step()
    arm = linucb.take_action(context)
    print('i:',i)
#     while reward[arm] == 0.0:
#         arm = arm - 1
    print('reward[arm]:',reward[arm],'arm:',arm)
    X_t = np.random.choice([0,1],p=[1-reward[arm], reward[arm]])
#     X_t = random.choices([0, 1], weights=[1-reward[arm], reward[arm]], k=1)[0]
#     NeuralTSregret += [NeuralTSregret[-1] + max(reward) - reward[arm]]
    LinUCBregret = LinUCBregret +  max(reward) - reward[arm]
    LinUCBregret_list.append(LinUCBregret)
#     linucb.update(context, arm, reward[arm])
    linucb.update(context, arm, X_t)
    LinUCBreward += reward[arm]
    LinUCBreward_list.append(LinUCBreward)


# plt.plot(LinUCBregret, label='LinUCB')
# print('linucb:', LinUCBregret[-1])

import matplotlib.pyplot as plt
import numpy as np

t = np.arange(0,10000,1)

# plt.title('g=3,d=2,k=100,N=100')
plt.plot(t, LinUCBreward_list[0:10000], color='red', label = 'Lin_UCB',linestyle="--",linewidth=2)
# plt.plot(t, regret_linucb_list, color='green', label = 'Linucb',linestyle="--",linewidth=2)
# plt.plot(t, regret_glinucb_list, color='blue', label = 'GLinucb',linestyle="--",linewidth=2)
plt.legend()
plt.grid()
plt.xlabel('Frame Number')
plt.ylabel('Cumulative Regret')
# plt.savefig('g=3,d=2,k=100,N=100.png', format='png')
plt.show()



import matplotlib.pyplot as plt
import numpy as np

t = np.arange(0,10000,1)

# plt.title('g=3,d=2,k=100,N=100')
plt.plot(t, NeuralUCBregret_list[0:10000], color='red', label = 'Neural_UCB',linestyle="--",linewidth=2)
plt.plot(t, LinUCBregret_list[0:10000], color='green', label = 'Linucb',linestyle="--",linewidth=2)
# plt.plot(t, regret_glinucb_list, color='blue', label = 'GLinucb',linestyle="--",linewidth=2)
plt.legend()
plt.grid()
plt.xlabel('Frame Number')
plt.ylabel('Cumulative Regret')
# plt.savefig('g=3,d=2,k=100,N=100.png', format='png')
plt.show()

"""# route 3"""

def get_reward(X_n):
    A = 4000
    reward = []

    # 繁忙时期 qubit较少 a>b>c>d

    # route a, 1 hop, 2 edges
    # 单跳单qubit单次纠缠成功率
    pe_c_1 = 2e-4
    pe_c_2 = 2e-4
    pe_c_3 = 2e-4
    # 单跳单qubit单slot(A次)纠缠成功率
    p_c_1 = 1 - (1 - pe_c_1)**A
    p_c_2 = 1 - (1 - pe_c_2)**A
    p_c_3 = 1 - (1 - pe_c_3)**A
#     # 单跳qubit数量
#     n_a_1 = 4
#     n_a_2 = 4
    # 单跳成功率
    for j in range(len(X_n)):
#         P_a_1 = 1 - (1 - p_a_1)**n_a_1
#         P_a_2 = 1 - (1 - p_a_2)**n_a_2
        P_c_1 = 1 - (1 - p_c_1)**X_n[j][0]
        P_c_2 = 1 - (1 - p_c_2)**X_n[j][1]
        P_c_3 = 1 - (1 - p_c_3)**X_n[j][2]
        # route a 成功率
        P_c = P_c_1 * P_c_2 * P_c_3
        reward.append(P_c)
    return reward

from env import *
from neuralucb import *
import matplotlib.pyplot as plt
import torch
from tqdm import trange
import random

X_n = []
qubit = 8
for i in range(qubit+1):
    for j in range(qubit+1-i):
        new_d = np.array([i,j,qubit-i-j])
        X_n.append(new_d)
X_n = np.array(X_n)

# for i in range(3):
#     front = np.zeros((4 * i))
#     back = np.zeros((4 * (2 - i)))
#     new_d = np.concatenate((front, x, back), axis=0)
#     X_n.append(new_d)
# X_n = np.array(X_n)

neuralucb = NeuralUCB(3, len(X_n), beta=1, lamb=1)
NeuralUCBregret_list = []
NeuralUCBregret = 0
# NeuralTSregret = [0]

context = X_n
reward = get_reward(X_n)
print('max:',max(reward),'id:',reward.index(max(reward)),'max_stra:',X_n[reward.index(max(reward))])
print(X_n)
print(reward)
for i in trange(10000):
#     context, reward = env.step()
    arm = neuralucb.take_action(context)
#     X_t = np.random.binomial(1, reward[arm])
    X_t = np.random.choice([0,1],p=[1-reward[arm], reward[arm]])
#     X_t = random.choices([0, 1], weights=[1-reward[arm], reward[arm]], k=1)[0]
#     NeuralTSregret += [NeuralTSregret[-1] + max(reward) - reward[arm]]
    NeuralUCBregret = NeuralUCBregret +  max(reward) - reward[arm]
    NeuralUCBregret_list.append(NeuralUCBregret)
#     neuralucb.update(context, arm, reward[arm])
    neuralucb.update(context, arm, X_t)


# plt.plot(NeuralUCBregret, label='NeuralUCB')
# print('neuralucb:', NeuralUCBregret[-1])

import matplotlib.pyplot as plt
import numpy as np

t = np.arange(0,2000,1)

# plt.title('g=3,d=2,k=100,N=100')
plt.plot(t, NeuralUCBregret_list[0:2000], color='red', label = 'Neural_UCB',linestyle="--",linewidth=2)
# plt.plot(t, regret_linucb_list, color='green', label = 'Linucb',linestyle="--",linewidth=2)
# plt.plot(t, regret_glinucb_list, color='blue', label = 'GLinucb',linestyle="--",linewidth=2)
plt.legend()
plt.grid()
plt.xlabel('Frame Number')
plt.ylabel('Cumulative Regret')
# plt.savefig('g=3,d=2,k=100,N=100.png', format='png')
plt.show()

linucb = LinUCB(3, len(X_n), beta=0.2, lamb=1)
# LinUCBregret = [0]

LinUCBregret = 0
LinUCBregret_list = []

for i in range(10000):
#     context, reward = env.step()
    arm = linucb.take_action(context)
    X_t = np.random.choice([0,1],p=[1-reward[arm], reward[arm]])
#     X_t = random.choices([0, 1], weights=[1-reward[arm], reward[arm]], k=1)[0]
#     NeuralTSregret += [NeuralTSregret[-1] + max(reward) - reward[arm]]
    LinUCBregret = LinUCBregret +  max(reward) - reward[arm]
    LinUCBregret_list.append(LinUCBregret)
#     linucb.update(context, arm, reward[arm])
    linucb.update(context, arm, X_t)


# plt.plot(LinUCBregret, label='LinUCB')
# print('linucb:', LinUCBregret[-1])

import matplotlib.pyplot as plt
import numpy as np

t = np.arange(0,10000,1)

# plt.title('g=3,d=2,k=100,N=100')
plt.plot(t, NeuralUCBregret_list[0:10000], color='red', label = 'Neural_UCB',linestyle="--",linewidth=2)
plt.plot(t, LinUCBregret_list[0:10000], color='green', label = 'Linucb',linestyle="--",linewidth=2)
# plt.plot(t, regret_glinucb_list, color='blue', label = 'GLinucb',linestyle="--",linewidth=2)
plt.legend()
plt.grid()
plt.xlabel('Frame Number')
plt.ylabel('Cumulative Regret')
# plt.savefig('g=3,d=2,k=100,N=100.png', format='png')
plt.show()

