{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title-cell"
   },
   "source": [
    "# Neural Bandit Algorithm Evaluation Framework\n",
    "\n",
    "## Graduate Research Project\n",
    "**AI & Quantum Computing Laboratory**  \n",
    "**Rochester Institute of Technology**\n",
    "\n",
    "---\n",
    "\n",
    "## Research Framework Overview\n",
    "\n",
    "This comprehensive evaluation framework provides rigorous analysis of neural bandit algorithms with clear categorical distinction between different operational environments:\n",
    "\n",
    "- **Baseline Environment**: Optimal performance benchmark (Oracle)\n",
    "- **Stochastic Environment**: Natural random failures and network noise\n",
    "- **Adversarial Environment**: Strategic intelligent attacks and malicious targeting\n",
    "\n",
    "## Primary Research Questions\n",
    "\n",
    "1. **Algorithm Robustness**: How do neural bandit algorithms perform across different threat models?\n",
    "2. **Comparative Analysis**: Which algorithms demonstrate superior performance in specific scenarios?\n",
    "3. **Quantified Performance**: What are the exact degradation metrics under adversarial conditions?\n",
    "4. **Theoretical Validation**: Do experimental results align with established regret bounds?\n",
    "\n",
    "## Key Research Contributions\n",
    "\n",
    "- **Systematic Environment Categorization**: Clear baseline/stochastic/adversarial taxonomy\n",
    "- **Multi-Algorithm Comparative Testing**: Comprehensive evaluation across 6+ algorithms\n",
    "- **Quantified Robustness Metrics**: Precise performance degradation measurements\n",
    "- **Publication-Ready Analysis**: Academic-quality visualizations and statistical validation\n",
    "\n",
    "## Evaluation Methodology\n",
    "\n",
    "The framework implements standardized testing protocols across three distinct categories:\n",
    "- **Baseline**: Oracle performance establishing theoretical upper bounds\n",
    "- **Stochastic**: Random environmental perturbations modeling realistic conditions  \n",
    "- **Adversarial**: Strategic attack scenarios simulating malicious interference\n",
    "\n",
    "Each algorithm undergoes identical testing conditions enabling direct performance comparison and robustness quantification across all operational environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-cell"
   },
   "source": [
    "## Environment Setup & Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1758879364121,
     "user": {
      "displayName": "Piter Garcia",
      "userId": "06279433864365870614"
     },
     "user_tz": 240
    },
    "id": "setup-code",
    "outputId": "4f884d74-a478-4776-8dbb-d758a770bacf"
   },
   "outputs": [],
   "source": [
    "# @title Quantum MAB Models Evaluation Framework - Environment Setup\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get current directory\n",
    "cur_dir = os.getcwd()\n",
    "print(f\"Current working directory: {cur_dir.split('/')[-1]}\")\n",
    "\n",
    "try:\n",
    "    import google.colab                 # Check if running in Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Change to framework directory (updated path)\n",
    "    project_dir = '/content/drive/MyDrive/GA-Work/hybrid_variable_framework/Dynamic_Routing_Eval_Framework'\n",
    "    os.chdir(project_dir)\n",
    "    print(\"Running in Google Colab\")\n",
    "\n",
    "    # Add source directory to path\n",
    "    project_code_dir = os.path.join(project_dir, 'src')\n",
    "    sys.path.append(project_code_dir)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Running locally (not in Colab)\")\n",
    "    # For local development, assume current directory structure\n",
    "    pass\n",
    "\n",
    "print(f\"Now working from: {os.getcwd().split('/')[-1]}\")\n",
    "\n",
    "# Verify Python version and framework initialization\n",
    "import sys\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(\"Quantum MAB Models Evaluation Framework - Environment Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23415,
     "status": "ok",
     "timestamp": 1758879387570,
     "user": {
      "displayName": "Piter Garcia",
      "userId": "06279433864365870614"
     },
     "user_tz": 240
    },
    "id": "install-dependencies",
    "outputId": "d2d41839-5f53-4750-dab1-35d8b5aac320"
   },
   "outputs": [],
   "source": [
    "# @title Framework Dependencies Installation\n",
    "\n",
    "# Install core scientific computing libraries for MAB evaluation\n",
    "!pip install torch torchvision numpy matplotlib seaborn pandas tqdm scipy scikit-learn -q\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Framework dependencies installed successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(\"Quantum MAB Models Evaluation Framework - Ready for model testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "framework-overview"
   },
   "source": [
    "## Threat Model Classification Framework\n",
    "\n",
    "### Systematic Environment Taxonomy\n",
    "\n",
    "This research framework establishes precise categorical distinctions for quantum network evaluation environments, addressing previous ambiguity in threat model classification:\n",
    "\n",
    "### Environmental Categories\n",
    "\n",
    "| Environment | Implementation | Threat Characteristic | Research Application |\n",
    "|-------------|----------------|----------------------|---------------------|\n",
    "| **Baseline** | `none` | Deterministic optimal performance | Theoretical upper bound |\n",
    "| **Stochastic** | `stochastic`/`random` | Natural random failures | Realistic network conditions |\n",
    "| **Adversarial** | `markov` | Oblivious strategic attacks | Pattern-based targeting |\n",
    "| **Adversarial** | `adaptive` | Responsive strategic attacks | Feedback-driven targeting |\n",
    "| **Adversarial** | `onlineadaptive` | Real-time strategic attacks | Dynamic threat adaptation |\n",
    "\n",
    "### Research Contribution\n",
    "\n",
    "This framework addresses a critical gap in existing literature where random network failures were often conflated with intentional adversarial attacks. The systematic categorization enables:\n",
    "\n",
    "- **Precise Robustness Quantification**: Exact performance degradation measurements across threat categories\n",
    "- **Comparative Algorithm Analysis**: Direct performance comparison under identical threat conditions\n",
    "- **Theoretical Validation**: Empirical verification of regret bounds across different adversarial models\n",
    "- **Reproducible Research Standards**: Standardized evaluation protocols for quantum network algorithms\n",
    "\n",
    "### Methodological Significance\n",
    "\n",
    "Previous research often lacked clear distinction between stochastic and adversarial environments, limiting the ability to assess true algorithm robustness under intentional attacks versus natural network degradation. This framework provides the necessary precision for rigorous academic evaluation of quantum routing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1758879388611,
     "user": {
      "displayName": "Piter Garcia",
      "userId": "06279433864365870614"
     },
     "user_tz": 240
    },
    "id": "import-framework",
    "outputId": "83e65604-566a-40f1-9cde-c1e84e0e16af"
   },
   "outputs": [],
   "source": [
    "# @title Quantum MAB Models Evaluation Framework - Core Components Import\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add parent directory to path so notebook can find daqr\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Install pmdarima for time series analysis (used in some models)\n",
    "try:\n",
    "    import pmdarima as pm\n",
    "except ImportError:\n",
    "    !pip install pmdarima -q\n",
    "    import pmdarima as pm\n",
    "\n",
    "print(f\"Now working from: {os.getcwd().split('/')[-1]}\")\n",
    "\n",
    "# Import and reload framework components\n",
    "try:\n",
    "    # Core environment and algorithms\n",
    "    from daqr.core import network_environment\n",
    "    importlib.reload(network_environment)\n",
    "    \n",
    "    from daqr.algorithms import base_bandit\n",
    "    importlib.reload(base_bandit)\n",
    "    \n",
    "    from daqr.algorithms import neural_bandits\n",
    "    importlib.reload(neural_bandits)\n",
    "    \n",
    "    from daqr.algorithms import predictive_bandits\n",
    "    importlib.reload(predictive_bandits)\n",
    "    \n",
    "    # Framework evaluation and testing\n",
    "    from daqr.evaluation import experiment_runner\n",
    "    importlib.reload(experiment_runner)\n",
    "    \n",
    "    from daqr.evaluation import multi_run_evaluator\n",
    "    importlib.reload(multi_run_evaluator)\n",
    "    \n",
    "    from daqr.evaluation import visualizer\n",
    "    importlib.reload(visualizer)\n",
    "    \n",
    "    from experiments import stochastic_evaluation\n",
    "    importlib.reload(stochastic_evaluation)\n",
    "    \n",
    "    # Configuration\n",
    "    from daqr.config import experiment_config\n",
    "    importlib.reload(experiment_config)\n",
    "\n",
    "    print(\"‚úÖ Quantum MAB Models Evaluation Framework - All components imported successfully\")\n",
    "    \n",
    "    # Try to verify framework configuration\n",
    "    try:\n",
    "        from daqr.config.experiment_config import ExperimentConfiguration\n",
    "        print(f\"üìä Framework configuration loaded: ExperimentConfiguration\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  ExperimentConfiguration not available yet\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error encountered: {e}\")\n",
    "    print(\"üîß Manual module creation may be required\")\n",
    "\n",
    "# Display current module status\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODULE STATUS SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modules_to_check = [\n",
    "    ('daqr.core', 'network_environment'),\n",
    "    ('daqr.algorithms', 'base_bandit'),\n",
    "    ('daqr.algorithms', 'neural_bandits'),\n",
    "    ('daqr.algorithms', 'predictive_bandits'),\n",
    "    ('daqr.evaluation', 'experiment_runner'),\n",
    "    ('daqr.evaluation', 'multi_run_evaluator'),\n",
    "    ('daqr.evaluation', 'visualizer'),\n",
    "    ('experiments', 'stochastic_evaluation'),\n",
    "    ('daqr.config', 'experiment_config')\n",
    "]\n",
    "\n",
    "for package, module_name in modules_to_check:\n",
    "    try:\n",
    "        mod = __import__(f\"{package}.{module_name}\", fromlist=[module_name])\n",
    "        print(f\"‚úÖ {package}.{module_name:<30} - Loaded\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package}.{module_name:<30} - Not available\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for new qubit allocator module\n",
    "try:\n",
    "    from daqr.core import qubit_allocator\n",
    "    importlib.reload(qubit_allocator)\n",
    "    print(\"\\nüÜï Dynamic Qubit Allocation module loaded!\")\n",
    "    from daqr.core.qubit_allocator import DynamicQubitAllocator, ThompsonSamplingAllocator\n",
    "    print(\"   Available allocators: DynamicQubitAllocator, ThompsonSamplingAllocator\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  qubit_allocator module not found (expected if not yet created)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theoretical-background"
   },
   "source": [
    "## Theoretical Foundation and Algorithm Architecture\n",
    "\n",
    "### Algorithm Composition\n",
    "\n",
    "The **EXPNeuralUCB** algorithm integrates two complementary theoretical frameworks:\n",
    "\n",
    "- **EXP3 Component**: Exponential-weight group selection mechanism for adversarial robustness\n",
    "- **NeuralUCB Component**: Neural network-based arm selection with rigorous uncertainty quantification\n",
    "\n",
    "### Theoretical Performance Guarantees\n",
    "\n",
    "The algorithm maintains provable regret bounds under adversarial conditions:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[\\text{Regret}(T)] = O\\left(\\sqrt{KT\\log T} + \\sqrt{ST\\log T}\\right)\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(K\\): Number of available arms (quantum routing paths)\n",
    "- \\(S\\): Number of strategic groups (allocation strategies)  \n",
    "- \\(T\\): Time horizon (evaluation period)\n",
    "\n",
    "### Research Hypotheses\n",
    "\n",
    "This evaluation framework tests three primary hypotheses regarding algorithm performance and robustness:\n",
    "\n",
    "**H1: Adversarial Superiority**\n",
    "EXPNeuralUCB demonstrates measurably superior performance in adversarial environments compared to baseline neural bandit algorithms lacking adversarial robustness mechanisms.\n",
    "\n",
    "**H2: Bounded Performance Degradation**  \n",
    "Performance degradation under adversarial attacks remains within acceptable bounds (< 15% relative to stochastic performance), validating the algorithm's practical applicability.\n",
    "\n",
    "**H3: Consistent Ranking Stability**\n",
    "Algorithm performance rankings remain stable across varying adversarial attack intensities, indicating reliable robustness characteristics.\n",
    "\n",
    "### Experimental Validation Framework\n",
    "\n",
    "The theoretical predictions are empirically validated through systematic evaluation across the defined environmental categories, providing quantitative evidence for the algorithm's adversarial robustness properties while maintaining performance guarantees under standard stochastic conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1758879388611,
     "user": {
      "displayName": "Piter Garcia",
      "userId": "06279433864365870614"
     },
     "user_tz": 240
    },
    "id": "theoretical-setup",
    "outputId": "a6c0fc57-a152-45e1-9da6-5113e519b8fe"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path so notebook can find daqr\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from daqr.config import experiment_config\n",
    "\n",
    "importlib.reload(experiment_config)\n",
    "from daqr.config.experiment_config import ExperimentConfiguration\n",
    "\n",
    "# Initialize configuration\n",
    "config = ExperimentConfiguration()\n",
    "\n",
    "# Define models for comprehensive evaluation (expandable)\n",
    "# Access model lists from config\n",
    "models = config.NEURAL_MODELS  # or CONTEXTUAL_MODELS, INFORMED_CONTEXTUAL_MODELS\n",
    "\n",
    "\n",
    "# Framework-specific experimental configuration\n",
    "FRAMEWORK_CONFIG = {\n",
    "    # Testing Configuration (lightweight for development)\n",
    "    'test_mode': True,  # Set to False for final comprehensive evaluation\n",
    "    'base_frames': 4000,    # Quick testing (increase to 4000+ for final runs)\n",
    "    'exp_num': 3,  # Fast iteration (increase to 10+ for statistical significance)\n",
    "    'frame_step': 2000,\n",
    "    'models': models,\n",
    "    \n",
    "    # Production Configuration (for final evaluation)\n",
    "    'prod_frames': 4000,     # Full evaluation frames\n",
    "    'prod_experiments': 10,  # Statistical significance\n",
    "    'frame_steps': [],  # Multi-horizon testing\n",
    "    \n",
    "    # Primary evaluation focus\n",
    "    'main_env': 'stochastic',\n",
    "    'eval_mod': 'comprehensive',\n",
    "    'main_model': 'CEXPNeuralUCB',\n",
    "    \n",
    "    # Routing strategy configuration (NEW)\n",
    "    'routing_strategy': 'fixed',  # 'fixed' or 'dynamic'\n",
    "    'enable_routing_comparison': False,  # Enable for routing paper\n",
    "    \n",
    "    # Algorithm parameters (EXPNeuralUCB paper-compliant)\n",
    "    'alg_attrs': {\n",
    "        'lambda_reg': 1.0,      # Regularization parameter\n",
    "        'gamma': 0.1,           # EXP3 exploration rate  \n",
    "        'network_width': 128,   # Neural network width\n",
    "        'network_depth': 2,     # Neural network depth\n",
    "        'gradient_steps': 8,    # Gradient descent steps\n",
    "        'learning_rate': 1e-4   # Learning rate\n",
    "    },\n",
    "\n",
    "    # Environment parameters\n",
    "    'env_attrs': {\n",
    "        'intensity': 0.25,  # Natural failure rate for stochastic\n",
    "        'base_seed': 12345,\n",
    "        'reproducible': True\n",
    "    },\n",
    "\n",
    "    # Test scenarios for different evaluation modes\n",
    "    'scenarios': {\n",
    "        'exp_focus': ['stochastic'],  # Primary focus\n",
    "        'stochastic_vs_baseline': ['none', 'stochastic'], # Comparison\n",
    "        'comprehensive': ['none', 'stochastic', 'markov', 'adaptive'], # Full evaluation\n",
    "        'adversarial': ['markov', 'adaptive', 'onlineadaptive']  # Future use\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate frame steps\n",
    "for exp_id in range(0, FRAMEWORK_CONFIG['exp_num']):\n",
    "    FRAMEWORK_CONFIG['frame_steps'].append(\n",
    "        FRAMEWORK_CONFIG['base_frames'] + (FRAMEWORK_CONFIG['frame_step'] * exp_id)\n",
    "    )\n",
    "FRAMEWORK_CONFIG['capacity'] = 10000 # quantum paper default capacity\n",
    "\n",
    "# Dynamic configuration based on testing mode\n",
    "attack_intensity= FRAMEWORK_CONFIG['env_attrs']['intensity']\n",
    "frame_step = FRAMEWORK_CONFIG['frame_step']\n",
    "current_frames = (FRAMEWORK_CONFIG['base_frames'] if FRAMEWORK_CONFIG['test_mode'] \n",
    "                  else FRAMEWORK_CONFIG['prod_frames'])\n",
    "current_experiments = (FRAMEWORK_CONFIG['exp_num'] if FRAMEWORK_CONFIG['test_mode'] \n",
    "                       else FRAMEWORK_CONFIG['prod_experiments'])\n",
    "\n",
    "# Display current configuration\n",
    "print(\"=\" * 70)\n",
    "print(\"DYNAMIC ROUTING EVALUATION FRAMEWORK - CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Models to evaluate: {len(models)} total\")\n",
    "print(f\"Primary environment: {FRAMEWORK_CONFIG['main_env'].upper()}\")\n",
    "print(f\"Routing strategy: {FRAMEWORK_CONFIG['routing_strategy'].upper()}\")\n",
    "\n",
    "print(f\"\\nCURRENT SETTINGS ({'TESTING' if FRAMEWORK_CONFIG['test_mode'] else 'PRODUCTION'} MODE):\")\n",
    "print(f\"  ‚Ä¢ Frames per run: {current_frames}\")\n",
    "print(f\"  ‚Ä¢ Experiments per model: {current_experiments}\")\n",
    "print(f\"  ‚Ä¢ Expected runtime: {'~2-3 minutes' if FRAMEWORK_CONFIG['test_mode'] else '~30-45 minutes'}\")\n",
    "\n",
    "print(f\"\\nALGORITHM PARAMETERS:\")\n",
    "for key, value in FRAMEWORK_CONFIG['alg_attrs'].items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nENVIRONMENT PARAMETERS:\")\n",
    "for key, value in FRAMEWORK_CONFIG['env_attrs'].items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nAVAILABLE TEST SCENARIOS:\")\n",
    "for scenario_name, environments in FRAMEWORK_CONFIG['scenarios'].items():\n",
    "    print(f\"  ‚Ä¢ {scenario_name}: {environments}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEVELOPMENT STRATEGY:\")\n",
    "if FRAMEWORK_CONFIG['test_mode']:\n",
    "    print(\"‚úì TESTING MODE - Fast iteration for development and debugging\")\n",
    "    print(\"  Switch test_mode to False for final comprehensive evaluation\")\n",
    "else:\n",
    "    print(\"‚úì PRODUCTION MODE - Full statistical evaluation\")\n",
    "\n",
    "if FRAMEWORK_CONFIG['enable_routing_comparison']:\n",
    "    print(\"\\n‚úì ROUTING COMPARISON ENABLED\")\n",
    "    print(\"  Framework will compare Fixed vs Dynamic routing strategies\")\n",
    "    \n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úì Configuration loaded successfully - Ready for model evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stochastic-vs-adversarial"
   },
   "source": [
    "## Comparative Analysis Framework: Stochastic versus Adversarial Environments\n",
    "\n",
    "### Research Focus\n",
    "\n",
    "This evaluation constitutes the primary empirical contribution of the research: systematic quantification of algorithm performance across fundamentally different operational conditions that distinguish between natural system failures and intentional strategic attacks.\n",
    "\n",
    "### Environmental Characterization\n",
    "\n",
    "**Stochastic Environment**\n",
    "- **Operational Model**: Natural random failures representing realistic network degradation patterns\n",
    "- **Attack Distribution**: Probabilistic failures following uniform random distribution\n",
    "- **Research Significance**: Establishes baseline performance metrics under standard operational conditions\n",
    "\n",
    "**Adversarial Environment**  \n",
    "- **Operational Model**: Strategic intelligent attacks systematically targeting algorithmic decision-making processes\n",
    "- **Attack Distribution**: Adaptive targeting mechanisms that dynamically respond to observed algorithm behavior\n",
    "- **Research Significance**: Evaluates robustness under worst-case strategic threat scenarios\n",
    "\n",
    "### Experimental Predictions\n",
    "\n",
    "Based on the theoretical analysis and algorithm architecture, the following empirical outcomes are anticipated:\n",
    "\n",
    "**Performance Superiority Hypothesis**\n",
    "EXPNeuralUCB will demonstrate measurably superior performance retention in adversarial environments relative to baseline neural bandit algorithms lacking specialized adversarial robustness mechanisms.\n",
    "\n",
    "**Bounded Degradation Hypothesis**\n",
    "Performance degradation under adversarial conditions will remain within acceptable operational limits, specifically maintaining performance within 85% of stochastic environment baselines.\n",
    "\n",
    "**Stability Hypothesis**\n",
    "Algorithm performance rankings will exhibit stability across varying adversarial attack intensities, indicating consistent robustness characteristics rather than scenario-dependent performance fluctuations.\n",
    "\n",
    "### Research Methodology\n",
    "\n",
    "The comparative analysis employs identical experimental conditions across both environments, enabling precise quantification of performance degradation attributable to adversarial targeting while controlling for environmental variables and maintaining statistical rigor in the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   QUANTUM MAB ‚Äì CLEAN RESET + FRESH IMPORT PIPELINE\n",
    "# ============================================================\n",
    "# This block:\n",
    "#   ‚úì Frees all global model objects\n",
    "#   ‚úì Clears CUDA + Python memory\n",
    "#   ‚úì Reloads all DAQR modules cleanly\n",
    "#   ‚úì Ensures any user can re-run cells safely in Colab\n",
    "#\n",
    "# This is the correct shared-drive-safe way to avoid\n",
    "# stale state, GPU memory leaks, and broken reload logic.\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# os.chdir(\"/content/drive/Shareddrives/ai_quantum_computing/quantum_project\")\n",
    "# print(\"üìå Now in:\", os.getcwd())\n",
    "root = os.path.abspath(\"../..\")\n",
    "# Compute the absolute path of cleanup_state_duplicates.py\n",
    "cleanup_script = os.path.abspath(f\"{root}/cleanup_state_duplicates.py\")\n",
    "\n",
    "print(f\"üöø Running cleanup script at:\\n{cleanup_script}\\n\")\n",
    "\n",
    "# Execute the script exactly as-is\n",
    "result = subprocess.run(\n",
    "    [\"python3\", cleanup_script],\n",
    "    text=True,\n",
    "    capture_output=True\n",
    ")\n",
    "\n",
    "print(\"===== CLEANUP STDOUT =====\")\n",
    "print(result.stdout)\n",
    "\n",
    "print(\"===== CLEANUP STDERR =====\")\n",
    "print(result.stderr)\n",
    "\n",
    "print(\"üöø Cleanup finished.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1 ‚Äî FULL CLEANUP\n",
    "# ============================================================\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def deep_cleanup():\n",
    "    \"\"\"\n",
    "    Remove all instantiated model/evaluator objects from global scope\n",
    "    and fully clear CPU/GPU memory.\n",
    "    \"\"\"\n",
    "    to_clear = [\n",
    "        \"oracle\", \"gneuralucb\", \"expneuralucb\",\n",
    "        \"cpursuitneuralucb\", \"icpursuitneuralucb\",\n",
    "        \"evaluator\", \"results\"\n",
    "    ]\n",
    "\n",
    "    for name in to_clear:\n",
    "        if name in globals():\n",
    "            obj = globals().get(name, None)\n",
    "            # If a cleanup() method exists, call it\n",
    "            try:\n",
    "                if hasattr(obj, \"cleanup\"):\n",
    "                    obj.cleanup(verbose=False)\n",
    "            except:\n",
    "                pass\n",
    "            # Remove the variable\n",
    "            globals().pop(name, None)\n",
    "\n",
    "    # GPU cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Force Python garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    # Reset default dtype to ensure numerical stability\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "    print(\"‚úì Deep cleanup complete (memory cleared)\")\n",
    "\n",
    "\n",
    "deep_cleanup()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2 ‚Äî CLEAN MODULE RELOAD\n",
    "# ============================================================\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# Ensure daqr package is discoverable\n",
    "PARENT_DIR = os.path.abspath(\"..\")\n",
    "if PARENT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PARENT_DIR)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Import DAQR modules\n",
    "# ------------------------------------------------------------\n",
    "from daqr.config import experiment_config, gd_backup_manager, local_backup_manager\n",
    "from daqr.core import network_environment, qubit_allocator\n",
    "from daqr.algorithms import neural_bandits, predictive_bandits, base_bandit\n",
    "from daqr.evaluation import multi_run_evaluator, visualizer, experiment_runner\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reload modules to ensure a clean environment\n",
    "# ------------------------------------------------------------\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(network_environment)\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(base_bandit)\n",
    "importlib.reload(neural_bandits)\n",
    "importlib.reload(predictive_bandits)\n",
    "importlib.reload(experiment_runner)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "importlib.reload(visualizer)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Import specific classes after reloads\n",
    "# ------------------------------------------------------------\n",
    "from daqr.config.gd_backup_manager import GoogleDriveBackupManager\n",
    "from daqr.config.experiment_config import ExperimentConfiguration\n",
    "from daqr.config.local_backup_manager import LocalBackupManager\n",
    "from daqr.core.network_environment import *\n",
    "from daqr.core.qubit_allocator import *\n",
    "from daqr.algorithms.base_bandit import *\n",
    "from daqr.algorithms.neural_bandits import *\n",
    "from daqr.algorithms.predictive_bandits import *\n",
    "from daqr.evaluation.multi_run_evaluator import *\n",
    "from daqr.evaluation.experiment_runner import *\n",
    "from daqr.evaluation.visualizer import QuantumEvaluatorVisualizer\n",
    "\n",
    "print(\"‚úì All modules reloaded successfully (fresh environment ready)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation scenarios based on framework focus\n",
    "if FRAMEWORK_CONFIG['main_env'] == 'stochastic':\n",
    "    # Primary stochastic evaluation with optional comparison\n",
    "    test_scenarios = {\n",
    "        'stochastic': 'Stochastic Random Failures',\n",
    "        'markov': 'Markov Adversarial Attack',\n",
    "        'adaptive': 'Adaptive Adversarial Attack',\n",
    "        'onlineadaptive': 'Online Adaptive Attack',\n",
    "        'none': 'Baseline (Optimal Conditions)'  # For comparison\n",
    "    }\n",
    "    evaluation_type = \"STOCHASTIC-FOCUSED\"\n",
    "else:\n",
    "    # Fallback to stochastic vs adversarial for comparison\n",
    "    test_scenarios = {\n",
    "        'stochastic': 'Stochastic (Natural Network Failures)', \n",
    "        'adaptive': 'Adversarial (Strategic Attacks)'\n",
    "    }\n",
    "    evaluation_type = \"COMPARATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.qubit_allocator import *\n",
    "from daqr.evaluation.multi_run_evaluator import *\n",
    "from daqr.config.experiment_config import ExperimentConfiguration\n",
    "\n",
    "# Create allocator with correct parameters\n",
    "allocator = ThompsonSamplingAllocator(\n",
    "    total_qubits=35,           # Total qubits available (8+10+8+9=35)\n",
    "    num_routes=4,              # Number of routes\n",
    "    min_qubits_per_route=2     # Minimum qubits per route\n",
    ")\n",
    "\n",
    "# # AFTER (UCB - Dynamic):\n",
    "# allocator = DynamicQubitAllocator(              # ‚Üê Change class name\n",
    "#     total_qubits=35,           \n",
    "#     num_routes=4,              \n",
    "#     min_qubits_per_route=2,\n",
    "#     exploration_bonus=2.0      # ‚Üê NEW parameter (optional, default is 2.0)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# Fully random (exploration only)\n",
    "# allocator = RandomQubitAllocator(epsilon=1.0, seed=42)\n",
    "\n",
    "# # 50-50 mix of random and baseline\n",
    "# random_50 = RandomQubitAllocator(epsilon=0.5, seed=42)\n",
    "\n",
    "# # Mostly baseline with occasional exploration (epsilon-greedy style)\n",
    "# random_10 = RandomQubitAllocator(epsilon=0.1, seed=42)\n",
    "\n",
    "# # Decaying randomness (start exploratory, become deterministic)\n",
    "# decaying = RandomQubitAllocator(\n",
    "#     epsilon=1.0,           # Start fully random\n",
    "#     epsilon_decay=0.99,    # Decay 1% per timestep\n",
    "#     min_epsilon=0.05,      # Eventually stabilize at 5% exploration\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# # Fully deterministic (acts like fixed allocator)\n",
    "# deterministic = RandomQubitAllocator(epsilon=0.0, seed=42)\n",
    "\n",
    "# models = ['Oracle', 'GNeuralUCB', 'EXPNeuralUCB']\n",
    "# allocator = None\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"QUANTUM MAB MODELS EVALUATION FRAMEWORK - STOCHASTIC FOCUS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "current_experiments = 5 # overwrite number of experiments\n",
    "last_backup = True\n",
    "overwrite = False\n",
    "base_cap = True\n",
    "\n",
    "# Create config with allocator\n",
    "custom_config = ExperimentConfiguration(\n",
    "    runs=current_experiments, allocator=allocator, \n",
    "    env_type=FRAMEWORK_CONFIG['main_env'], scenarios=test_scenarios, use_last_backup=last_backup,\n",
    "    models=models, attack_intensity=attack_intensity, scale=1, base_capacity=base_cap, overwrite=overwrite)\n",
    "evaluator = MultiRunEvaluator(configs=custom_config, base_frames=current_frames, frame_step=frame_step)\n",
    "\n",
    "# -----------------------------\n",
    "# START LOGGING (UNIQUE NAME)\n",
    "# -----------------------------\n",
    "evaluator.configs.set_log_name(base_frames=current_frames, frame_step=frame_step)\n",
    "evaluator.configs.backup_mgr.init_logging_redirect(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Quantum MAB Models Stochastic Evaluation - Primary Framework Execution\n",
    "print(\"\\n‚úì Framework Configuration:\")\n",
    "print(f\"  ‚Ä¢ Primary Environment: {FRAMEWORK_CONFIG['main_env'].upper()}\")\n",
    "print(f\"  ‚Ä¢ Evaluation Mode: {FRAMEWORK_CONFIG['eval_mod'].upper()}\")\n",
    "print(f\"  ‚Ä¢ Models to Test: {len(models)}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚ñ∂ Executing {evaluation_type.upper()} EVALUATION:\")\n",
    "for scenario, description in test_scenarios.items():\n",
    "    print(f\"  ‚Ä¢ {scenario.upper():<20} {description}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Execute framework evaluation\n",
    "try:\n",
    "    print(\"\\n‚öô Running Quantum MAB Models Evaluation...\")\n",
    "    comparison_results = evaluator.test_stochastic_environment(cal_winner=True, parellel=False)\n",
    "    evaluator.calculate_scenarios_performance()\n",
    "    last_exp_comparison_results = evaluator.get_evaluation_results(FRAMEWORK_CONFIG['main_env'])\n",
    "    print(f\"\\n‚úì Quantum MAB Models Evaluation Framework - {evaluation_type.upper()} evaluation completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Evaluation error: {e}\")\n",
    "    print(\"‚ö† This may indicate missing framework components or configuration issues\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # ------------------------------------\n",
    "    # ALWAYS STOP LOGGING CLEANLY\n",
    "    # ------------------------------------\n",
    "    evaluator.configs.backup_mgr.load_new_entries()\n",
    "    evaluator.configs.backup_mgr.stop_logging_redirect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "robustness-analysis",
    "outputId": "d5bf77ec-ed26-474f-b68b-650ab683fece"
   },
   "outputs": [],
   "source": [
    "# @title Robustness Analysis and Quantification\n",
    "import importlib\n",
    "# from daqr.evaluation import visualizer\n",
    "\n",
    "importlib.reload(visualizer)\n",
    "from daqr.evaluation.visualizer import QuantumEvaluatorVisualizer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Pretty print comparison results\n",
    "    import pprint\n",
    "    print(\"Comparison Results Summary:\")\n",
    "    # pprint.pprint(comparison_results)\n",
    "\n",
    "    # Full comparison plot (all scenarios together)\n",
    "    viz = QuantumEvaluatorVisualizer(comparison_results, allocator=allocator, config=custom_config)\n",
    "    viz.plot_stochastic_vs_adversarial_comparison()\n",
    "\n",
    "    # Get list of all scenarios\n",
    "    scenario_list = list(test_scenarios.keys())\n",
    "\n",
    "    # Plot each scenario individually\n",
    "    for scenario in scenario_list:\n",
    "        if scenario.lower() == 'stochastic': pass\n",
    "        print(f\"\\nüìä Generating plots for scenario: {scenario.upper()}\")\n",
    "        evaluator.calculate_scenario_performance(scenario=scenario)\n",
    "\n",
    "        # Get ALL results for this scenario (all experiments)\n",
    "        all_scenario_results = evaluator.get_evaluation_results(scenario=scenario)\n",
    "        \n",
    "        # Just pass the scenario name - method auto-detects and plots it\n",
    "        viz.plot_scenarios_comparison(scenario=scenario)\n",
    "        \n",
    "        # # Also plot just the last experiment\n",
    "        # if len(all_scenario_results[scenario].keys()) > 1:\n",
    "        #     last_scenario_results = evaluator.get_evaluation_results(scenario=scenario,exp_id=-1)\n",
    "        #     if last_scenario_results: viz.plot_scenarios_comparison(last_scenario_results)\n",
    "\n",
    "    print(\"\\n All scenario plots generated!\")\n",
    "\n",
    "    \n",
    "    print(\"\\n‚úì Stochastic Analysis Generated:\")\n",
    "    print(\"  ‚Üí quantum_mab_models_stochastic_evaluation.png\")\n",
    "    \n",
    "    # Use viz.get_viz_data() to access pre-computed averaged results\n",
    "    stoch_data = viz.get_viz_data(f'stochastic_data')\n",
    "    \n",
    "    if stoch_data and 'averaged' in stoch_data:\n",
    "        stoch_results = stoch_data['averaged']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STOCHASTIC PERFORMANCE METRICS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        oracle_reward = stoch_results.get('oracle_reward', 1)\n",
    "        winner = stoch_results.get('winner', 'N/A')\n",
    "        \n",
    "        for alg in models:\n",
    "            if alg in stoch_results['results']:\n",
    "                model_data = stoch_results['results'][alg]\n",
    "                \n",
    "                # Use PRE-COMPUTED metrics\n",
    "                stoch_reward = model_data.get('final_reward', 0)\n",
    "                efficiency = model_data.get('efficiency', 0)\n",
    "                gap = model_data.get('gap', float('inf'))\n",
    "                \n",
    "                print(f\"\\n{alg}:\")\n",
    "                print(f\"  ‚Ä¢ Stochastic Performance: {stoch_reward:.3f}\")\n",
    "                print(f\"  ‚Ä¢ Oracle Efficiency: {efficiency:.1f}%\")\n",
    "                print(f\"  ‚Ä¢ Oracle Gap: {gap:.1f}%\")\n",
    "                \n",
    "                if efficiency > 90:     classification = \"EXCELLENT\"\n",
    "                elif efficiency > 80:   classification = \"GOOD\"\n",
    "                elif efficiency > 70:   classification = \"MODERATE\"\n",
    "                else:                   classification = \"NEEDS IMPROVEMENT\"\n",
    "                \n",
    "                print(f\"  ‚Ä¢ Classification: {classification}\")\n",
    "                \n",
    "                if alg == winner:\n",
    "                    print(f\"  ‚òÖ WINNER ‚òÖ\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STOCHASTIC ENVIRONMENT INSIGHTS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"  ‚Ä¢ Natural quantum decoherence and network failures\")\n",
    "        print(\"  ‚Ä¢ Performance metrics validate theoretical predictions\")\n",
    "        print(\"  ‚Ä¢ Baseline for future adversarial robustness studies\")\n",
    "    else:\n",
    "        print(\"‚ö† No stochastic averaged results available\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in robustness analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.qubit_allocator import *\n",
    "from daqr.evaluation.multi_run_evaluator import *\n",
    "from daqr.config.experiment_config import ExperimentConfiguration\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"QUANTUM MAB MODELS EVALUATION FRAMEWORK - STOCHASTIC FOCUS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create config with allocator\n",
    "custom_config = ExperimentConfiguration(\n",
    "    runs=current_experiments, allocator=allocator, \n",
    "    env_type=FRAMEWORK_CONFIG['main_env'], scenarios=test_scenarios, use_last_backup=last_backup,\n",
    "    models=models, attack_intensity=attack_intensity, scale=1.5, base_capacity=base_cap, overwrite=overwrite) \n",
    "evaluator = MultiRunEvaluator(configs=custom_config, base_frames=current_frames, frame_step=frame_step)\n",
    "\n",
    "# -----------------------------\n",
    "# START LOGGING (UNIQUE NAME)\n",
    "# -----------------------------\n",
    "evaluator.configs.set_log_name(base_frames=current_frames, frame_step=frame_step)\n",
    "evaluator.configs.backup_mgr.init_logging_redirect(evaluator)\n",
    "\n",
    "# -----------------------------\n",
    "# START LOGGING (UNIQUE NAME)\n",
    "# -----------------------------\n",
    "evaluator.configs.set_log_name(base_frames=current_frames, frame_step=frame_step)\n",
    "evaluator.configs.backup_mgr.init_logging_redirect(evaluator)\n",
    "\n",
    "# @title Quantum MAB Models Stochastic Evaluation - Primary Framework Execution\n",
    "print(\"\\n‚úì Framework Configuration:\")\n",
    "print(f\"  ‚Ä¢ Primary Environment: {FRAMEWORK_CONFIG['main_env'].upper()}\")\n",
    "print(f\"  ‚Ä¢ Evaluation Mode: {FRAMEWORK_CONFIG['eval_mod'].upper()}\")\n",
    "print(f\"  ‚Ä¢ Models to Test: {len(models)}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚ñ∂ Executing {evaluation_type.upper()} EVALUATION:\")\n",
    "for scenario, description in test_scenarios.items():\n",
    "    print(f\"  ‚Ä¢ {scenario.upper():<20} {description}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Execute framework evaluation\n",
    "try:\n",
    "    print(\"\\n‚öô Running Quantum MAB Models Evaluation...\")\n",
    "    comparison_results = evaluator.test_stochastic_environment(cal_winner=True, parellel=False)\n",
    "    evaluator.calculate_scenarios_performance()\n",
    "    last_exp_comparison_results = evaluator.get_evaluation_results(FRAMEWORK_CONFIG['main_env'])\n",
    "    print(f\"\\n‚úì Quantum MAB Models Evaluation Framework - {evaluation_type.upper()} evaluation completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Evaluation error: {e}\")\n",
    "    print(\"‚ö† This may indicate missing framework components or configuration issues\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # ------------------------------------\n",
    "    # ALWAYS STOP LOGGING CLEANLY\n",
    "    # ------------------------------------\n",
    "    evaluator.configs.backup_mgr.load_new_entries()\n",
    "    evaluator.configs.backup_mgr.stop_logging_redirect()\n",
    "\n",
    "\n",
    "# @title Robustness Analysis and Quantification\n",
    "import importlib\n",
    "# from daqr.evaluation import visualizer\n",
    "\n",
    "importlib.reload(visualizer)\n",
    "from daqr.evaluation.visualizer import QuantumEvaluatorVisualizer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Pretty print comparison results\n",
    "    import pprint\n",
    "    print(\"Comparison Results Summary:\")\n",
    "    # pprint.pprint(comparison_results)\n",
    "\n",
    "    # Full comparison plot (all scenarios together)\n",
    "    viz = QuantumEvaluatorVisualizer(comparison_results, allocator=allocator, config=custom_config)\n",
    "    viz.plot_stochastic_vs_adversarial_comparison()\n",
    "\n",
    "    # Get list of all scenarios\n",
    "    scenario_list = list(test_scenarios.keys())\n",
    "\n",
    "    # Plot each scenario individually\n",
    "    for scenario in scenario_list:\n",
    "        if scenario.lower() == 'stochastic': pass\n",
    "        print(f\"\\nüìä Generating plots for scenario: {scenario.upper()}\")\n",
    "        evaluator.calculate_scenario_performance(scenario=scenario)\n",
    "\n",
    "        # Get ALL results for this scenario (all experiments)\n",
    "        all_scenario_results = evaluator.get_evaluation_results(scenario=scenario)\n",
    "        \n",
    "        # Just pass the scenario name - method auto-detects and plots it\n",
    "        viz.plot_scenarios_comparison(scenario=scenario)\n",
    "        \n",
    "        # # Also plot just the last experiment\n",
    "        # if len(all_scenario_results[scenario].keys()) > 1:\n",
    "        #     last_scenario_results = evaluator.get_evaluation_results(scenario=scenario,exp_id=-1)\n",
    "        #     if last_scenario_results: viz.plot_scenarios_comparison(last_scenario_results)\n",
    "\n",
    "    print(\"\\n All scenario plots generated!\")\n",
    "\n",
    "    \n",
    "    print(\"\\n‚úì Stochastic Analysis Generated:\")\n",
    "    print(\"  ‚Üí quantum_mab_models_stochastic_evaluation.png\")\n",
    "    \n",
    "    # Use viz.get_viz_data() to access pre-computed averaged results\n",
    "    stoch_data = viz.get_viz_data(f'stochastic_data')\n",
    "    \n",
    "    if stoch_data and 'averaged' in stoch_data:\n",
    "        stoch_results = stoch_data['averaged']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STOCHASTIC PERFORMANCE METRICS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        oracle_reward = stoch_results.get('oracle_reward', 1)\n",
    "        winner = stoch_results.get('winner', 'N/A')\n",
    "        \n",
    "        for alg in models:\n",
    "            if alg in stoch_results['results']:\n",
    "                model_data = stoch_results['results'][alg]\n",
    "                \n",
    "                # Use PRE-COMPUTED metrics\n",
    "                stoch_reward = model_data.get('final_reward', 0)\n",
    "                efficiency = model_data.get('efficiency', 0)\n",
    "                gap = model_data.get('gap', float('inf'))\n",
    "                \n",
    "                print(f\"\\n{alg}:\")\n",
    "                print(f\"  ‚Ä¢ Stochastic Performance: {stoch_reward:.3f}\")\n",
    "                print(f\"  ‚Ä¢ Oracle Efficiency: {efficiency:.1f}%\")\n",
    "                print(f\"  ‚Ä¢ Oracle Gap: {gap:.1f}%\")\n",
    "                \n",
    "                if efficiency > 90:     classification = \"EXCELLENT\"\n",
    "                elif efficiency > 80:   classification = \"GOOD\"\n",
    "                elif efficiency > 70:   classification = \"MODERATE\"\n",
    "                else:                   classification = \"NEEDS IMPROVEMENT\"\n",
    "                \n",
    "                print(f\"  ‚Ä¢ Classification: {classification}\")\n",
    "                \n",
    "                if alg == winner:\n",
    "                    print(f\"  ‚òÖ WINNER ‚òÖ\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STOCHASTIC ENVIRONMENT INSIGHTS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"  ‚Ä¢ Natural quantum decoherence and network failures\")\n",
    "        print(\"  ‚Ä¢ Performance metrics validate theoretical predictions\")\n",
    "        print(\"  ‚Ä¢ Baseline for future adversarial robustness studies\")\n",
    "    else:\n",
    "        print(\"‚ö† No stochastic averaged results available\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in robustness analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.qubit_allocator import *\n",
    "from daqr.evaluation.multi_run_evaluator import *\n",
    "from daqr.config.experiment_config import ExperimentConfiguration\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"QUANTUM MAB MODELS EVALUATION FRAMEWORK - STOCHASTIC FOCUS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create config with allocator\n",
    "custom_config = ExperimentConfiguration(\n",
    "    runs=current_experiments, allocator=allocator, \n",
    "    env_type=FRAMEWORK_CONFIG['main_env'], scenarios=test_scenarios, use_last_backup=last_backup,\n",
    "    models=models, attack_intensity=attack_intensity, scale=2, base_capacity=base_cap, overwrite=overwrite) # 4-8k<-cap=8k, 10-12k<-cap=20k, 14-18k<-cap=28k, 20-24k<-cap=40k\n",
    "evaluator = MultiRunEvaluator(configs=custom_config, base_frames=current_frames, frame_step=frame_step)\n",
    "\n",
    "# -----------------------------\n",
    "# START LOGGING (UNIQUE NAME)\n",
    "# -----------------------------\n",
    "evaluator.configs.set_log_name(base_frames=current_frames, frame_step=frame_step)\n",
    "evaluator.configs.backup_mgr.init_logging_redirect(evaluator)\n",
    "\n",
    "# -----------------------------\n",
    "# START LOGGING (UNIQUE NAME)\n",
    "# -----------------------------\n",
    "evaluator.configs.set_log_name(base_frames=current_frames, frame_step=frame_step)\n",
    "evaluator.configs.backup_mgr.init_logging_redirect(evaluator)\n",
    "\n",
    "# @title Quantum MAB Models Stochastic Evaluation - Primary Framework Execution\n",
    "print(\"\\n‚úì Framework Configuration:\")\n",
    "print(f\"  ‚Ä¢ Primary Environment: {FRAMEWORK_CONFIG['main_env'].upper()}\")\n",
    "print(f\"  ‚Ä¢ Evaluation Mode: {FRAMEWORK_CONFIG['eval_mod'].upper()}\")\n",
    "print(f\"  ‚Ä¢ Models to Test: {len(models)}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚ñ∂ Executing {evaluation_type.upper()} EVALUATION:\")\n",
    "for scenario, description in test_scenarios.items():\n",
    "    print(f\"  ‚Ä¢ {scenario.upper():<20} {description}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Execute framework evaluation\n",
    "try:\n",
    "    print(\"\\n‚öô Running Quantum MAB Models Evaluation...\")\n",
    "    comparison_results = evaluator.test_stochastic_environment(cal_winner=True, parellel=False)\n",
    "    evaluator.calculate_scenarios_performance()\n",
    "    last_exp_comparison_results = evaluator.get_evaluation_results(FRAMEWORK_CONFIG['main_env'])\n",
    "    print(f\"\\n‚úì Quantum MAB Models Evaluation Framework - {evaluation_type.upper()} evaluation completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Evaluation error: {e}\")\n",
    "    print(\"‚ö† This may indicate missing framework components or configuration issues\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # ------------------------------------\n",
    "    # ALWAYS STOP LOGGING CLEANLY\n",
    "    # ------------------------------------\n",
    "    evaluator.configs.backup_mgr.load_new_entries()\n",
    "    evaluator.configs.backup_mgr.stop_logging_redirect()\n",
    "\n",
    "\n",
    "# @title Robustness Analysis and Quantification\n",
    "import importlib\n",
    "# from daqr.evaluation import visualizer\n",
    "\n",
    "importlib.reload(visualizer)\n",
    "from daqr.evaluation.visualizer import QuantumEvaluatorVisualizer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Pretty print comparison results\n",
    "    import pprint\n",
    "    print(\"Comparison Results Summary:\")\n",
    "    # pprint.pprint(comparison_results)\n",
    "\n",
    "    # Full comparison plot (all scenarios together)\n",
    "    viz = QuantumEvaluatorVisualizer(comparison_results, allocator=allocator, config=custom_config)\n",
    "    viz.plot_stochastic_vs_adversarial_comparison()\n",
    "\n",
    "    # Get list of all scenarios\n",
    "    scenario_list = list(test_scenarios.keys())\n",
    "\n",
    "    # Plot each scenario individually\n",
    "    for scenario in scenario_list:\n",
    "        if scenario.lower() == 'stochastic': pass\n",
    "        print(f\"\\nüìä Generating plots for scenario: {scenario.upper()}\")\n",
    "        evaluator.calculate_scenario_performance(scenario=scenario)\n",
    "\n",
    "        # Get ALL results for this scenario (all experiments)\n",
    "        all_scenario_results = evaluator.get_evaluation_results(scenario=scenario)\n",
    "        \n",
    "        # Just pass the scenario name - method auto-detects and plots it\n",
    "        viz.plot_scenarios_comparison(scenario=scenario)\n",
    "        \n",
    "        # # Also plot just the last experiment\n",
    "        # if len(all_scenario_results[scenario].keys()) > 1:\n",
    "        #     last_scenario_results = evaluator.get_evaluation_results(scenario=scenario,exp_id=-1)\n",
    "        #     if last_scenario_results: viz.plot_scenarios_comparison(last_scenario_results)\n",
    "\n",
    "    print(\"\\n All scenario plots generated!\")\n",
    "\n",
    "    \n",
    "    print(\"\\n‚úì Stochastic Analysis Generated:\")\n",
    "    print(\"  ‚Üí quantum_mab_models_stochastic_evaluation.png\")\n",
    "    \n",
    "    # Use viz.get_viz_data() to access pre-computed averaged results\n",
    "    stoch_data = viz.get_viz_data(f'stochastic_data')\n",
    "    \n",
    "    if stoch_data and 'averaged' in stoch_data:\n",
    "        stoch_results = stoch_data['averaged']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STOCHASTIC PERFORMANCE METRICS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        oracle_reward = stoch_results.get('oracle_reward', 1)\n",
    "        winner = stoch_results.get('winner', 'N/A')\n",
    "        \n",
    "        for alg in models:\n",
    "            if alg in stoch_results['results']:\n",
    "                model_data = stoch_results['results'][alg]\n",
    "                \n",
    "                # Use PRE-COMPUTED metrics\n",
    "                stoch_reward = model_data.get('final_reward', 0)\n",
    "                efficiency = model_data.get('efficiency', 0)\n",
    "                gap = model_data.get('gap', float('inf'))\n",
    "                \n",
    "                print(f\"\\n{alg}:\")\n",
    "                print(f\"  ‚Ä¢ Stochastic Performance: {stoch_reward:.3f}\")\n",
    "                print(f\"  ‚Ä¢ Oracle Efficiency: {efficiency:.1f}%\")\n",
    "                print(f\"  ‚Ä¢ Oracle Gap: {gap:.1f}%\")\n",
    "                \n",
    "                if efficiency > 90:     classification = \"EXCELLENT\"\n",
    "                elif efficiency > 80:   classification = \"GOOD\"\n",
    "                elif efficiency > 70:   classification = \"MODERATE\"\n",
    "                else:                   classification = \"NEEDS IMPROVEMENT\"\n",
    "                \n",
    "                print(f\"  ‚Ä¢ Classification: {classification}\")\n",
    "                \n",
    "                if alg == winner:\n",
    "                    print(f\"  ‚òÖ WINNER ‚òÖ\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STOCHASTIC ENVIRONMENT INSIGHTS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"  ‚Ä¢ Natural quantum decoherence and network failures\")\n",
    "        print(\"  ‚Ä¢ Performance metrics validate theoretical predictions\")\n",
    "        print(\"  ‚Ä¢ Baseline for future adversarial robustness studies\")\n",
    "    else:\n",
    "        print(\"‚ö† No stochastic averaged results available\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in robustness analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comprehensive-evaluation"
   },
   "source": [
    "## Multi-Environment Performance Analysis\n",
    "\n",
    "### Complete Evaluation Matrix\n",
    "\n",
    "This research extends beyond the primary stochastic-adversarial comparison to provide comprehensive algorithm assessment across the complete spectrum of operational environments, establishing a thorough empirical foundation for robustness evaluation.\n",
    "\n",
    "### Environmental Test Framework\n",
    "\n",
    "| Environment | Classification | Threat Characteristics | Analytical Purpose |\n",
    "|-------------|---------------|----------------------|-------------------|\n",
    "| `none` | Baseline | Deterministic optimal conditions | Theoretical performance ceiling |\n",
    "| `stochastic` | Probabilistic | Uniform random failures | Standard operational baseline |\n",
    "| `markov` | Adversarial | Memory-dependent strategic attacks | Oblivious adversarial model |\n",
    "| `adaptive` | Adversarial | Feedback-driven strategic attacks | Responsive adversarial model |\n",
    "| `onlineadaptive` | Adversarial | Real-time adaptive strategic attacks | Sophisticated adversarial model |\n",
    "\n",
    "### Research Contributions\n",
    "\n",
    "**Comprehensive Threat Model Coverage**\n",
    "The evaluation framework addresses the complete spectrum of operational conditions, from optimal deterministic environments through increasingly sophisticated adversarial scenarios, providing unprecedented coverage of realistic deployment conditions.\n",
    "\n",
    "**Graduated Adversarial Complexity Analysis**  \n",
    "The systematic progression from oblivious to sophisticated adversarial models enables precise quantification of algorithm performance degradation as threat sophistication increases, revealing critical robustness thresholds.\n",
    "\n",
    "**Cross-Environment Validation Protocol**\n",
    "Consistent algorithm ranking across multiple environments validates robustness claims and identifies algorithms with stable performance characteristics independent of operational conditions.\n",
    "\n",
    "**Empirical Robustness Quantification**\n",
    "The multi-environment approach enables precise measurement of performance degradation rates, establishing quantitative robustness metrics that support theoretical predictions and practical deployment decisions.\n",
    "\n",
    "### Methodological Significance\n",
    "\n",
    "This comprehensive evaluation protocol addresses limitations in existing literature where algorithm assessment often focuses on narrow operational scenarios, providing the empirical foundation necessary for robust algorithm deployment in practical quantum network environments."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1rknAIThhNzWIoGwNHJR_N0F6hBb7T3e-",
     "timestamp": 1759053280580
    }
   ]
  },
  "kernelspec": {
   "display_name": ".quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
