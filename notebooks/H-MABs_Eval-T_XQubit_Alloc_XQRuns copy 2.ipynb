{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title-cell"
   },
   "source": [
    "# Neural Bandit Algorithm Evaluation Framework\n",
    "\n",
    "## Graduate Research Project\n",
    "**AI & Quantum Computing Laboratory**  \n",
    "**Rochester Institute of Technology**\n",
    "\n",
    "---\n",
    "\n",
    "## Research Framework Overview\n",
    "\n",
    "This comprehensive evaluation framework provides rigorous analysis of neural bandit algorithms with clear categorical distinction between different operational environments:\n",
    "\n",
    "- **Baseline Environment**: Optimal performance benchmark (Oracle)\n",
    "- **Stochastic Environment**: Natural random failures and network noise\n",
    "- **Adversarial Environment**: Strategic intelligent attacks and malicious targeting\n",
    "\n",
    "## Primary Research Questions\n",
    "\n",
    "1. **Algorithm Robustness**: How do neural bandit algorithms perform across different threat models?\n",
    "2. **Comparative Analysis**: Which algorithms demonstrate superior performance in specific scenarios?\n",
    "3. **Quantified Performance**: What are the exact degradation metrics under adversarial conditions?\n",
    "4. **Theoretical Validation**: Do experimental results align with established regret bounds?\n",
    "\n",
    "## Key Research Contributions\n",
    "\n",
    "- **Systematic Environment Categorization**: Clear baseline/stochastic/adversarial taxonomy\n",
    "- **Multi-Algorithm Comparative Testing**: Comprehensive evaluation across 6+ algorithms\n",
    "- **Quantified Robustness Metrics**: Precise performance degradation measurements\n",
    "- **Publication-Ready Analysis**: Academic-quality visualizations and statistical validation\n",
    "\n",
    "## Evaluation Methodology\n",
    "\n",
    "The framework implements standardized testing protocols across three distinct categories:\n",
    "- **Baseline**: Oracle performance establishing theoretical upper bounds\n",
    "- **Stochastic**: Random environmental perturbations modeling realistic conditions  \n",
    "- **Adversarial**: Strategic attack scenarios simulating malicious interference\n",
    "\n",
    "Each algorithm undergoes identical testing conditions enabling direct performance comparison and robustness quantification across all operational environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "framework-overview"
   },
   "source": [
    "## Threat Model Classification Framework\n",
    "\n",
    "### Systematic Environment Taxonomy\n",
    "\n",
    "This research framework establishes precise categorical distinctions for quantum network evaluation environments, addressing previous ambiguity in threat model classification:\n",
    "\n",
    "### Environmental Categories\n",
    "\n",
    "| Environment | Implementation | Threat Characteristic | Research Application |\n",
    "|-------------|----------------|----------------------|---------------------|\n",
    "| **Baseline** | `none` | Deterministic optimal performance | Theoretical upper bound |\n",
    "| **Stochastic** | `stochastic`/`random` | Natural random failures | Realistic network conditions |\n",
    "| **Adversarial** | `markov` | Oblivious strategic attacks | Pattern-based targeting |\n",
    "| **Adversarial** | `adaptive` | Responsive strategic attacks | Feedback-driven targeting |\n",
    "| **Adversarial** | `onlineadaptive` | Real-time strategic attacks | Dynamic threat adaptation |\n",
    "\n",
    "### Research Contribution\n",
    "\n",
    "This framework addresses a critical gap in existing literature where random network failures were often conflated with intentional adversarial attacks. The systematic categorization enables:\n",
    "\n",
    "- **Precise Robustness Quantification**: Exact performance degradation measurements across threat categories\n",
    "- **Comparative Algorithm Analysis**: Direct performance comparison under identical threat conditions\n",
    "- **Theoretical Validation**: Empirical verification of regret bounds across different adversarial models\n",
    "- **Reproducible Research Standards**: Standardized evaluation protocols for quantum network algorithms\n",
    "\n",
    "### Methodological Significance\n",
    "\n",
    "Previous research often lacked clear distinction between stochastic and adversarial environments, limiting the ability to assess true algorithm robustness under intentional attacks versus natural network degradation. This framework provides the necessary precision for rigorous academic evaluation of quantum routing algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-cell"
   },
   "source": [
    "## Environment Setup & Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1758879388611,
     "user": {
      "displayName": "Piter Garcia",
      "userId": "06279433864365870614"
     },
     "user_tz": 240
    },
    "id": "theoretical-setup",
    "outputId": "a6c0fc57-a152-45e1-9da6-5113e519b8fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: notebooks\n",
      "Running locally (not in Colab)\n",
      "Now working from: notebooks\n",
      "Framework dependencies installed successfully\n",
      "Python version: 3.12.1\n",
      "PyTorch version: 2.8.0+cu128\n",
      "NumPy version: 1.26.4\n",
      "NetworkX version: 3.5\n",
      "Matplotlib version: 3.10.6\n",
      "Quantum MAB Models Evaluation Framework - Ready for Paper 7 (QBGP) testing\n",
      "\n",
      "üöø Running cleanup script at: /workspaces/quantum_project/cleanup_state_duplicates.py\n",
      "\n",
      "===== CLEANUP STDOUT =====\n",
      " Script dir: /workspaces/quantum_project\n",
      "Project root: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework\n",
      "\n",
      "State roots:\n",
      "  ‚úÖ /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state\n",
      "  ‚úÖ /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state\n",
      "  ‚ùå /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/framework_state\n",
      "  ‚ùå /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/model_state\n",
      "\n",
      "========== STARTING CLEANUP ==========\n",
      "\n",
      "[CLEANUP] Removing .0 float artifacts from filenames...\n",
      "[‚úì] Float artifacts fixed: 0\n",
      "\n",
      "--- Cleaning: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state ---\n",
      "[CLEAN] /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state\n",
      "[‚úì] /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state: removed 0 duplicates\n",
      "\n",
      "--- Cleaning: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state ---\n",
      "[CLEAN] /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state\n",
      "[‚úì] /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state: removed 0 duplicates\n",
      "\n",
      "--- Cleaning: /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/framework_state ---\n",
      "[SKIP] Missing path ‚Üí /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/framework_state\n",
      "\n",
      "--- Cleaning: /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/model_state ---\n",
      "[SKIP] Missing path ‚Üí /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/model_state\n",
      "\n",
      "\n",
      "========== CONSOLIDATION: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state ==========\n",
      "Target date directory: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state/day_20260130\n",
      "[‚úì] Consolidation finished for /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state: moved 0 files, removed 0 directories.\n",
      "\n",
      "\n",
      "========== CONSOLIDATION: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state ==========\n",
      "Target date directory: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state/day_20260130\n",
      "[‚úì] Consolidation finished for /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state: moved 0 files, removed 0 directories.\n",
      "[SKIP] Missing path ‚Üí /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/framework_state\n",
      "[SKIP] Missing path ‚Üí /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/model_state\n",
      "\n",
      "[UPDATE] Updating registry: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/local_backup_registry.json\n",
      "[DEBUG] State roots passed in:\n",
      "        - /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state  (exists=True)\n",
      "        - /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state  (exists=True)\n",
      "\n",
      "[DEBUG] COMPONENT: framework_state\n",
      "        root = /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state\n",
      "\n",
      "[DEBUG] COMPONENT: model_state\n",
      "        root = /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state\n",
      "[‚úì] Registry updated: 21 corrected, 0 added\n",
      "[INFO] Registry not found, skipping update: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/drive_backup_registry.json\n",
      "[INFO] Registry not found, skipping update: /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake/backup_registry.json\n",
      "\n",
      "========== DONE ==========\n",
      "\n",
      "\n",
      "===== CLEANUP STDERR =====\n",
      " /workspaces/quantum_project/cleanup_state_duplicates.py:1338: SyntaxWarning: invalid escape sequence '\\('\n",
      "  \"scenario_cat\": re.split(\"\\(|\\)\", scenario_cat)[-2],\n",
      "\n",
      "‚úì Deep cleanup complete (memory cleared)\n",
      "PyTorch version: 2.8.0+cu128\n",
      "NumPy version: 1.26.4\n",
      "Using device: cpu\n",
      "PyTorch version: 2.8.0+cu128\n",
      "NumPy version: 1.26.4\n",
      "Using device: cpu\n",
      "‚úì All modules reloaded successfully (Paper 7 environment ready)\n",
      "PyTorch version: 2.8.0+cu128\n",
      "NumPy version: 1.26.4\n",
      "Using device: cpu\n",
      "PyTorch version: 2.8.0+cu128\n",
      "NumPy version: 1.26.4\n",
      "Using device: cpu\n",
      "\n",
      "===================== GET LOCAL REGISTRY =====================\n",
      "‚Üí Skipping local cache (force=True or file missing)\n",
      "\n",
      "===================== BUILD LOCAL REGISTRY =====================\n",
      "\n",
      "================================================================================\n",
      "üîç SCANNING LOCAL FILES\n",
      "================================================================================\n",
      "Parameters: load_to_drive=False, force=False\n",
      "\n",
      "üìÇ Checking DRIVE mode: /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake\n",
      "   ‚ö†Ô∏è  Path does not exist, skipping\n",
      "\n",
      "üìÇ Checking LOCAL mode: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config\n",
      "   ‚úÖ Path exists, starting walk...\n",
      "   üö∂ Walking directory tree: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config\n",
      "\n",
      "   üìÅ Directory: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state/day_20260130\n",
      "      Relative: model_state/day_20260130\n",
      "      Component: model_state\n",
      "      Date: day_20260130\n",
      "      üìÑ Processing 19 files in model_state/day_20260130\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "      üìä model_state/day_20260130: 19/19 files processed\n",
      "      üìä model_state/day_20260130: 0/19 files skipped\n",
      "      üìä model_state/day_20260130: 0/19 files conflicted\n",
      "\n",
      "   üìÅ Directory: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state/day_20260130\n",
      "      Relative: framework_state/day_20260130\n",
      "      Component: framework_state\n",
      "      Date: day_20260130\n",
      "      üìÑ Processing 2 files in framework_state/day_20260130\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "      üìä framework_state/day_20260130: 2/2 files processed\n",
      "      üìä framework_state/day_20260130: 0/2 files skipped\n",
      "      üìä framework_state/day_20260130: 0/2 files conflicted\n",
      "\n",
      "   üìä Processed 2 directories\n",
      "\n",
      "================================================================================\n",
      "üì¶ BUILDING FINAL REGISTRY\n",
      "================================================================================\n",
      "Components found: ['model_state', 'framework_state']\n",
      "  ‚Ä¢ model_state: 19 files\n",
      "  ‚Ä¢ framework_state: 2 files\n",
      "\n",
      "‚úÖ SCAN COMPLETE\n",
      "Modes scanned: 1\n",
      "Total files in registry: 21\n",
      "================================================================================\n",
      "\n",
      "\t‚Üí Filesystem scan found 21 files\n",
      "\n",
      "===================== GET DRIVE REGISTRY =====================\n",
      "===================== REGISTRY BUILD COMPLETE =====================\n",
      "\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "\tüì¶ LocalBackupManager Registry Saved Locally in LOCAL\n",
      "BUILDING REGISTRY WITH 0 EXPECTED COMPONENTS KEYS\n",
      "\n",
      "===================== GET LOCAL REGISTRY =====================\n",
      "‚Üí Skipping local cache (force=True or file missing)\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "\tüì¶ LocalBackupManager Registry Saved Locally in LOCAL\n",
      "======================================================================\n",
      "DYNAMIC ROUTING EVALUATION FRAMEWORK - PAPER 7 (QBGP) CONFIGURATION\n",
      "======================================================================\n",
      "Primary Testbed: Paper 7 (Liu et al., 2024) - QBGP Multi-ISP Routing\n",
      "Models to evaluate: 5 total\n",
      "Topology: Real AS data\n",
      "AS Nodes: 50\n",
      "ISP Nodes: 3\n",
      "Paths per ISP pair: 5-shortest\n",
      "Reward Mode: neg_hop\n",
      "Context-aware Rewards: True\n",
      "\n",
      "‚úì Configuration loaded successfully - Ready for Paper 7 evaluation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Setup: Quantum MAB Framework (Paper 7 QBGP Testbed)\n",
    "# ============================================================\n",
    "\n",
    "# --- Install Dependencies ---\n",
    "!pip install -q torch torchvision numpy matplotlib seaborn pandas tqdm scipy scikit-learn pmdarima networkx\n",
    "\n",
    "# --- Core Imports ---\n",
    "import os, sys, gc, warnings, importlib, subprocess\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Path Setup ---\n",
    "print(f\"Current working directory: {os.getcwd().split('/')[-1]}\")\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    project_dir = '/content/drive/MyDrive/GA-Work/hybrid_variable_framework/Dynamic_Routing_Eval_Framework'\n",
    "    os.chdir(project_dir)\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    print(\"Running locally (not in Colab)\")\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "print(f\"Now working from: {os.getcwd().split('/')[-1]}\")\n",
    "\n",
    "# --- Framework Verification ---\n",
    "print(\"Framework dependencies installed successfully\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(\"Quantum MAB Models Evaluation Framework - Ready for Paper 7 (QBGP) testing\")\n",
    "\n",
    "# --- Cleanup Script Execution ---\n",
    "root = os.path.abspath(\"../..\")\n",
    "cleanup_script = os.path.join(root, \"cleanup_state_duplicates.py\")\n",
    "if os.path.exists(cleanup_script):\n",
    "    print(f\"\\nüöø Running cleanup script at: {cleanup_script}\\n\")\n",
    "    result = subprocess.run([\"python3\", cleanup_script], text=True, capture_output=True)\n",
    "    print(\"===== CLEANUP STDOUT =====\\n\", result.stdout)\n",
    "    print(\"===== CLEANUP STDERR =====\\n\", result.stderr)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cleanup script not found, skipping...\")\n",
    "\n",
    "# --- Deep Cleanup ---\n",
    "def deep_cleanup():\n",
    "    to_clear = [\"oracle\", \"gneuralucb\", \"expneuralucb\", \"cpursuitneuralucb\",\n",
    "                \"icpursuitneuralucb\", \"evaluator\", \"results\"]\n",
    "    for name in to_clear:\n",
    "        if name in globals():\n",
    "            obj = globals().get(name)\n",
    "            if hasattr(obj, \"cleanup\"): obj.cleanup(verbose=False)\n",
    "            globals().pop(name, None)\n",
    "    gc.collect()\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    print(\"‚úì Deep cleanup complete (memory cleared)\")\n",
    "\n",
    "deep_cleanup()\n",
    "\n",
    "# Ensure daqr package is discoverable\n",
    "PARENT_DIR = os.path.abspath(\"..\")\n",
    "if PARENT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PARENT_DIR)\n",
    "\n",
    "# --- Final Module Setup ---\n",
    "from daqr.core.qubit_allocator              import (QubitAllocator, RandomQubitAllocator, DynamicQubitAllocator, ThompsonSamplingAllocator)\n",
    "from daqr.core.quantum_physics              import (MemoryNoiseModel, FullPaper2FidelityCalculator, Paper2RewardFunction)\n",
    "from daqr.evaluation                        import experiment_runner, multi_run_evaluator, visualizer, allocator_runner\n",
    "from daqr.config                            import experiment_config, gd_backup_manager, local_backup_manager\n",
    "from daqr.algorithms                        import base_bandit, neural_bandits, predictive_bandits\n",
    "from daqr.core                              import network_environment, qubit_allocator\n",
    "from daqr.evaluation.visualizer             import QuantumEvaluatorVisualizer\n",
    "from daqr.config.gd_backup_manager          import GoogleDriveBackupManager\n",
    "from daqr.config.experiment_config          import ExperimentConfiguration\n",
    "from experiments                            import stochastic_evaluation\n",
    "from daqr.config.local_backup_manager       import LocalBackupManager\n",
    "from daqr.core.network_environment          import *\n",
    "from daqr.core.qubit_allocator              import *\n",
    "from daqr.algorithms.base_bandit            import *\n",
    "from daqr.algorithms.neural_bandits         import *\n",
    "from daqr.algorithms.predictive_bandits     import *\n",
    "from daqr.evaluation.multi_run_evaluator    import *\n",
    "from daqr.evaluation.experiment_runner      import *\n",
    "\n",
    "# ============================================================================\n",
    "# PAPER-SPECIFIC IMPORTS\n",
    "# ============================================================================\n",
    "from daqr.core.topology_generator           import Paper2TopologyGenerator\n",
    "from daqr.core.topology_generator           import Paper7ASTopologyGenerator  # üÜï Paper 7\n",
    "from daqr.core.topology_generator           import Paper12WaxmanTopologyGenerator\n",
    "from daqr.core.quantum_physics              import FiberLossNoiseModel, CascadedFidelityCalculator\n",
    "from daqr.core.quantum_physics              import FusionNoiseModel, FusionFidelityCalculator, QuARCRewardFunction\n",
    "from daqr.core.quantum_physics              import Paper12RetryFidelityCalculator\n",
    "from daqr.core.quantum_physics              import Paper7RewardFunction  # üÜï Paper 7\n",
    "from daqr.core                              import attack_strategy\n",
    "\n",
    "print(\"‚úì All modules reloaded successfully (Paper 7 environment ready)\")\n",
    "\n",
    "# --- Config & Model Setup ---\n",
    "for module in [experiment_config, network_environment, qubit_allocator, attack_strategy, base_bandit, \n",
    "               neural_bandits, predictive_bandits, experiment_runner, multi_run_evaluator, visualizer, \n",
    "               stochastic_evaluation]:\n",
    "    importlib.reload(module)\n",
    "\n",
    "config = ExperimentConfiguration()\n",
    "models = config.NEURAL_MODELS\n",
    "\n",
    "# ============================================================================\n",
    "# FRAMEWORK CONFIGURATION (Paper 7 Optimized)\n",
    "# ============================================================================\n",
    "FRAMEWORK_CONFIG = {\n",
    "    'exp_num': 5,\n",
    "    'test_mode': True,\n",
    "    'base_frames': 4000,\n",
    "    'frame_step': 2000,\n",
    "    'models': models,\n",
    "    'intensity': 0.25,\n",
    "    'routing_strategy': 'fixed',\n",
    "    'capacity': 10000,\n",
    "    'main_env': 'stochastic',\n",
    "\n",
    "    # Environment parameters\n",
    "    'env_attrs': {\n",
    "        'intensity': 0.25,\n",
    "        'base_seed': 12345,\n",
    "        'reproducible': True\n",
    "    },\n",
    "\n",
    "    'default': {\n",
    "        'num_paths': 4,\n",
    "        'total_qubits': 35,\n",
    "        'min_qubits_per_route': 2,\n",
    "        'exploration_bonus': 2.0,\n",
    "        'epsilon': 1.0,\n",
    "        'seed': 42\n",
    "    },\n",
    "    \n",
    "    # üéØ Paper #7 (Liu et al. 2024 - QBGP) - PRIMARY TESTBED\n",
    "    'paper7': {\n",
    "        # Topology Configuration\n",
    "        'k': 5,                      # k-shortest paths per ISP pair\n",
    "        'n_qisps': 3,                # Number of quantum ISP nodes\n",
    "        'num_paths': 15,              # Total paths for framework compatibility\n",
    "        'max_nodes': 50,             # AS subgraph size (30-80 for testing, None for full)\n",
    "        'network_scale': 'small',    # 'small' (30-50), 'medium' (100-200), 'large' (full)\n",
    "        'topology_path': '/Users/pitergarcia/DataScience/Semester4/GA-Work/hybrid_variable_framework/Dynamic_Routing_Eval_Framework/daqr/core/topology_data/as20000101.txt',\n",
    "        \n",
    "        # Framework Parameters\n",
    "        'total_qubits': 75,\n",
    "        'min_qubits_per_route': 2,\n",
    "        'exploration_bonus': 2.0,\n",
    "        \n",
    "        # Context & Reward Configuration\n",
    "        'use_context_rewards': True,      # Enable context-aware reward function\n",
    "        'reward_mode': 'neg_hop',         # Options: 'neg_hop', 'neg_degree', 'neg_length'\n",
    "        'use_synthetic': False,           # Force synthetic topology (ignore topology_path)\n",
    "        \n",
    "        # Topology Processing\n",
    "        'largest_cc_only': True,          # Use largest connected component\n",
    "        'relabel_to_int': True,           # Relabel nodes to integers\n",
    "        \n",
    "        # Synthetic Fallback (if topology_path fails or use_synthetic=True)\n",
    "        'synthetic_kind': 'barabasi_albert',\n",
    "        'synthetic_params': {\n",
    "            'n': 50,                      # Number of nodes\n",
    "            'm': 3                        # Edges per new node\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Paper #2 (Huang et al. - Neural Bandit Work)\n",
    "    'paper2': {\n",
    "        # Topology & Paths\n",
    "        'num_paths': 4,\n",
    "        'num_total_qubits': 75,\n",
    "        'dest_node': 14,\n",
    "        'num_nodes': 15,\n",
    "        'source_node': 1,\n",
    "        \n",
    "        # Base Physics\n",
    "        'p_init': 0.00001,\n",
    "        'total_qubits': 75,\n",
    "        'f_attenuation': 0.05,\n",
    "        \n",
    "        # Stochastic Noise Parameters\n",
    "        'p_BSM': 0.2,\n",
    "        'p_GateErrors': 0.2,\n",
    "        'p_depol': 0.1,\n",
    "\n",
    "        # State Configuration\n",
    "        'testbed': 'paper2',\n",
    "        'initial_state': 'idle',\n",
    "        'state_total_qubits': {'busy': 35, 'idle': 43},\n",
    "        \n",
    "        # Bandit Algorithm\n",
    "        'exploration_bonus': 2.0,\n",
    "        'min_qubits_per_route': 2,\n",
    "        'transition_trigger': True,\n",
    "        'paper2_transition_interval': 50,\n",
    "        'entanglement_success_factor': 4000,\n",
    "        \n",
    "        # Paper2 Features\n",
    "        'use_paper2_rewards': True,\n",
    "        'swap_mode': 'async',\n",
    "        'memory_T2': 5000,\n",
    "        'gate_error_rate': 0.02,\n",
    "        'swap_delay_per_link': 100,\n",
    "        'use_gate_error': True,\n",
    "        'use_memory_decay': True,\n",
    "    },\n",
    "    \n",
    "    # Paper #12 (Wang et al. 2024 - QuARC)\n",
    "    'paper12': {\n",
    "        # Topology\n",
    "        'n_nodes': 100,\n",
    "        'avg_degree': 6,\n",
    "        'waxman_beta': 0.2,\n",
    "        'waxman_alpha': 0.4,\n",
    "        'topology_type': 'waxman',\n",
    "        \n",
    "        # Physical parameters\n",
    "        'channel_width': 3,\n",
    "        'fusion_prob': 0.9,\n",
    "        'qubits_per_node': 12,\n",
    "        'entanglement_prob': 0.6,\n",
    "        \n",
    "        # Simulation parameters\n",
    "        'num_sd_pairs': 10,\n",
    "        'epoch_length': 500,\n",
    "        'total_timeslots': 7000,\n",
    "        \n",
    "        # QuARC-specific\n",
    "        'split_constant': 4,\n",
    "        'enable_clustering': True,\n",
    "        'enable_secondary_fusions': True,\n",
    "        \n",
    "        # Framework mapping\n",
    "        'num_paths': 4,\n",
    "        'total_qubits': 120,\n",
    "        'exploration_bonus': 1.5,\n",
    "        'min_qubits_per_route': 3,\n",
    "        'use_fusion_rewards': True,\n",
    "\n",
    "        'time_decay_physics': {\n",
    "            'memory_lifetime': 0.5\n",
    "        },\n",
    "\n",
    "        # Retry parameters\n",
    "        'retry_threshold': 0.7,\n",
    "        'max_retry_attempts': 3,\n",
    "        'retry_decay_rate': 0.95,\n",
    "        'enable_retry_logging': True,\n",
    "        'retry_cost_per_attempt': 0.1,\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Test Scenarios ---\n",
    "if FRAMEWORK_CONFIG['main_env'] == 'stochastic':\n",
    "    test_scenarios = {\n",
    "        'none': 'Baseline (Optimal Conditions)',\n",
    "        'stochastic': 'Stochastic Random Failures',\n",
    "        'markov': 'Markov Adversarial Attack',\n",
    "        'adaptive': 'Adaptive Adversarial Attack',\n",
    "        'onlineadaptive': 'Online Adaptive Attack'\n",
    "    }\n",
    "    evaluation_type = \"STOCHASTIC-FOCUSED\"\n",
    "else:\n",
    "    test_scenarios = {\n",
    "        'stochastic': 'Stochastic (Natural Network Failures)',\n",
    "        'adaptive': 'Adversarial (Strategic Attacks)'\n",
    "    }\n",
    "    evaluation_type = \"COMPARATIVE\"\n",
    "\n",
    "# --- Display Configuration ---\n",
    "print(\"=\" * 70)\n",
    "print(\"DYNAMIC ROUTING EVALUATION FRAMEWORK - PAPER 7 (QBGP) CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Primary Testbed: Paper 7 (Liu et al., 2024) - QBGP Multi-ISP Routing\")\n",
    "print(f\"Models to evaluate: {len(models)} total\")\n",
    "print(f\"Topology: {'Real AS data' if not FRAMEWORK_CONFIG['paper7']['use_synthetic'] else 'Synthetic Barab√°si-Albert'}\")\n",
    "print(f\"AS Nodes: {FRAMEWORK_CONFIG['paper7']['max_nodes'] or 'Full network'}\")\n",
    "print(f\"ISP Nodes: {FRAMEWORK_CONFIG['paper7']['n_qisps']}\")\n",
    "print(f\"Paths per ISP pair: {FRAMEWORK_CONFIG['paper7']['k']}-shortest\")\n",
    "print(f\"Reward Mode: {FRAMEWORK_CONFIG['paper7']['reward_mode']}\")\n",
    "print(f\"Context-aware Rewards: {FRAMEWORK_CONFIG['paper7']['use_context_rewards']}\")\n",
    "print(\"\\n‚úì Configuration loaded successfully - Ready for Paper 7 evaluation\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stochastic-vs-adversarial"
   },
   "source": [
    "## Comparative Analysis Framework: Stochastic versus Adversarial Environments\n",
    "\n",
    "### Research Focus\n",
    "\n",
    "This evaluation constitutes the primary empirical contribution of the research: systematic quantification of algorithm performance across fundamentally different operational conditions that distinguish between natural system failures and intentional strategic attacks.\n",
    "\n",
    "### Environmental Characterization\n",
    "\n",
    "**Stochastic Environment**\n",
    "- **Operational Model**: Natural random failures representing realistic network degradation patterns\n",
    "- **Attack Distribution**: Probabilistic failures following uniform random distribution\n",
    "- **Research Significance**: Establishes baseline performance metrics under standard operational conditions\n",
    "\n",
    "**Adversarial Environment**  \n",
    "- **Operational Model**: Strategic intelligent attacks systematically targeting algorithmic decision-making processes\n",
    "- **Attack Distribution**: Adaptive targeting mechanisms that dynamically respond to observed algorithm behavior\n",
    "- **Research Significance**: Evaluates robustness under worst-case strategic threat scenarios\n",
    "\n",
    "### Experimental Predictions\n",
    "\n",
    "Based on the theoretical analysis and algorithm architecture, the following empirical outcomes are anticipated:\n",
    "\n",
    "**Performance Superiority Hypothesis**\n",
    "EXPNeuralUCB will demonstrate measurably superior performance retention in adversarial environments relative to baseline neural bandit algorithms lacking specialized adversarial robustness mechanisms.\n",
    "\n",
    "**Bounded Degradation Hypothesis**\n",
    "Performance degradation under adversarial conditions will remain within acceptable operational limits, specifically maintaining performance within 85% of stochastic environment baselines.\n",
    "\n",
    "**Stability Hypothesis**\n",
    "Algorithm performance rankings will exhibit stability across varying adversarial attack intensities, indicating consistent robustness characteristics rather than scenario-dependent performance fluctuations.\n",
    "\n",
    "### Research Methodology\n",
    "\n",
    "The comparative analysis employs identical experimental conditions across both environments, enabling precise quantification of performance degradation attributable to adversarial targeting while controlling for environmental variables and maintaining statistical rigor in the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç PAPER 7 QUICK VALIDATION\n",
      "======================================================================\n",
      "üìä Paper7 Topology: Real AS (as20000101.txt)\n",
      "üìä Paper7 Paths: 15 paths from 5-shortest between 3 ISPs\n",
      "üìä Paper7 Contexts: 15 context vectors generated\n",
      "üìä Paper7 Rewards: Context-aware (mode=neg_hop)\n",
      "‚è±Ô∏è  get_physics_params_paper7() time: 5.7 ms\n",
      "‚úÖ Topology: 50 nodes, 141 edges\n",
      "‚úÖ Contexts: 15 paths\n",
      "‚úÖ Rewards: Enabled\n",
      "\n",
      "‚úì Paper 7 integration validated successfully\n",
      "======================================================================\n",
      "\n",
      "üöÄ Ready to run Paper 7 (QBGP) experiments!\n",
      "   Example: PHYSICS_MODELS = ['paper7']\n",
      "            ALLOCATORS = ['ThompsonSampling', 'DynamicUCB']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PAPER 7 HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_paper7_paths(topology, k: int, n_qisps: int, seed: int):\n",
    "    \"\"\"\n",
    "    Generate k-shortest paths between n_qisps quantum ISP nodes.\n",
    "    \n",
    "    Args:\n",
    "        topology: NetworkX graph (AS-level topology)\n",
    "        k: Number of shortest paths per ISP pair\n",
    "        n_qisps: Number of quantum ISP nodes\n",
    "        seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        List of paths (each path is a list of nodes)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    nodes = list(topology.nodes())\n",
    "\n",
    "    if len(nodes) < n_qisps:\n",
    "        raise ValueError(f\"Topology has {len(nodes)} nodes, need {n_qisps} for ISPs\")\n",
    "    \n",
    "    # Select ISP nodes (prefer high-degree nodes like real BGP)\n",
    "    degrees = dict(topology.degree())\n",
    "    sorted_nodes = sorted(nodes, key=lambda n: degrees[n], reverse=True)\n",
    "    isp_nodes = sorted_nodes[:n_qisps]  # Take top-degree nodes\n",
    "    \n",
    "    all_paths = []\n",
    "    for src, dst in itertools.combinations(isp_nodes, 2):\n",
    "        try:\n",
    "            path_generator = nx.shortest_simple_paths(topology, src, dst, weight='distance')\n",
    "            paths = list(itertools.islice(path_generator, k))\n",
    "            all_paths.extend(paths)\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "    \n",
    "    if not all_paths:\n",
    "        raise RuntimeError(f\"Could not find any paths between {n_qisps} ISP nodes\")\n",
    "    \n",
    "    return all_paths\n",
    "\n",
    "\n",
    "def generate_paper7_contexts(paths, topology):\n",
    "    \"\"\"\n",
    "    Generate context vectors for each path: [hop_count, avg_degree, path_length].\n",
    "    \n",
    "    Args:\n",
    "        paths: List of paths (each path is a list of nodes)\n",
    "        topology: NetworkX graph\n",
    "        \n",
    "    Returns:\n",
    "        List of context arrays, one per path (shape: [1, 3])\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    \n",
    "    for path in paths:\n",
    "        # Feature 1: Hop count (AS path length)\n",
    "        hop_count = len(path) - 1\n",
    "        \n",
    "        # Feature 2: Average node degree (bottleneck indicator)\n",
    "        degrees = [topology.degree(node) for node in path]\n",
    "        avg_degree = sum(degrees) / len(degrees) if degrees else 0.0\n",
    "\n",
    "        # Feature 3: Physical path length (sum of edge distances)\n",
    "        path_length = 0.0\n",
    "        for i in range(len(path) - 1):\n",
    "            edge_data = topology.get_edge_data(path[i], path[i+1])\n",
    "            path_length += edge_data.get('distance', 1.0)\n",
    "\n",
    "        # Context vector: [hop_count, avg_degree, path_length]\n",
    "        context_vector = np.array([hop_count, avg_degree, path_length], dtype=float)\n",
    "        contexts.append([context_vector])  # Wrap in list for framework compatibility\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "\n",
    "def get_physics_params(\n",
    "    physics_model: str = \"default\",\n",
    "    current_frames: int = 4000,\n",
    "    base_seed: int = 42,\n",
    "    qubit_cap=None,\n",
    "    *,\n",
    "    topology: \"nx.Graph | None\" = None,\n",
    "    topology_model: str | None = None,\n",
    "    topology_path: str | Path | None = None,\n",
    "    topology_max_nodes: int | None = None,\n",
    "    topology_largest_cc_only: bool = True,\n",
    "    topology_relabel_to_int: bool = True,\n",
    "    synthetic_kind: str = \"barabasi_albert\",\n",
    "    synthetic_params: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified physics parameter generator for all testbeds.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {noise_model, fidelity_calculator, external_topology, \n",
    "               external_contexts, external_rewards}\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============================================================================\n",
    "    # üéØ PAPER 7 (QBGP) - PRIMARY TESTBED\n",
    "    # ============================================================================\n",
    "    if physics_model == \"paper7\":\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        paper7_cfg = FRAMEWORK_CONFIG['paper7']\n",
    "        node_num = paper7_cfg.get('max_nodes')\n",
    "\n",
    "        # --- Topology Generation ---\n",
    "        if topology is not None:\n",
    "            final_topology = topology\n",
    "            print(f\"üìä Paper7 Topology: User-provided ({len(topology.nodes())} nodes)\")\n",
    "        else:\n",
    "            # Determine if using synthetic or real AS data\n",
    "            if paper7_cfg.get('use_synthetic', False) or not paper7_cfg.get('topology_path'):\n",
    "                # Synthetic fallback\n",
    "                synth_params = synthetic_params or paper7_cfg.get('synthetic_params', {'n': 50, 'm': 3})\n",
    "                topo_gen = Paper7ASTopologyGenerator(\n",
    "                    edge_list_path=\"dummy_nonexistent.txt\",\n",
    "                    max_nodes=topology_max_nodes or node_num,\n",
    "                    seed=base_seed,\n",
    "                    synthetic_fallback=True,\n",
    "                    synthetic_kind=paper7_cfg.get('synthetic_kind', 'barabasi_albert'),\n",
    "                    synthetic_params=synth_params\n",
    "                )\n",
    "                print(f\"üìä Paper7 Topology: Synthetic ({paper7_cfg.get('synthetic_kind')}, n={synth_params.get('n', 50)})\")\n",
    "            else:\n",
    "                # Real AS topology\n",
    "                topo_gen = Paper7ASTopologyGenerator(\n",
    "                    edge_list_path=paper7_cfg['topology_path'],\n",
    "                    max_nodes=node_num,\n",
    "                    seed=base_seed,\n",
    "                    relabel_to_integers=paper7_cfg.get('relabel_to_int', True),\n",
    "                    largest_cc_only=paper7_cfg.get('largest_cc_only', True),\n",
    "                    synthetic_fallback=True\n",
    "                )\n",
    "                topo_path_short = paper7_cfg['topology_path'].split('/')[-1]\n",
    "                print(f\"üìä Paper7 Topology: Real AS ({topo_path_short})\")\n",
    "            \n",
    "            final_topology = topo_gen.generate()\n",
    "\n",
    "        # --- Path Generation ---\n",
    "        k = paper7_cfg[\"k\"]\n",
    "        n_qisps = paper7_cfg[\"n_qisps\"]\n",
    "        paths = generate_paper7_paths(final_topology, k, n_qisps, base_seed)\n",
    "        contexts = generate_paper7_contexts(paths, final_topology)\n",
    "        \n",
    "        elapsed_ms = (time.time() - start_time) * 1000\n",
    "        print(f\"üìä Paper7 Paths: {len(paths)} paths from {k}-shortest between {n_qisps} ISPs\")\n",
    "        print(f\"üìä Paper7 Contexts: {len(contexts)} context vectors generated\")\n",
    "\n",
    "        # --- Reward Function (Optional) ---\n",
    "        external_rewards = None\n",
    "        if paper7_cfg.get('use_context_rewards', False):\n",
    "            reward_mode = paper7_cfg.get('reward_mode', 'neg_hop')\n",
    "            reward_func = Paper7RewardFunction(mode=reward_mode)\n",
    "            external_rewards = []\n",
    "            for ctx_list in contexts:\n",
    "                path_rewards = [reward_func.compute(ctx) for ctx in ctx_list]\n",
    "                external_rewards.append(path_rewards)\n",
    "            print(f\"üìä Paper7 Rewards: Context-aware (mode={reward_mode})\")\n",
    "        else:\n",
    "            print(f\"üìä Paper7 Rewards: Using default framework rewards\")\n",
    "\n",
    "        print(f\"‚è±Ô∏è  get_physics_params_paper7() time: {elapsed_ms:.1f} ms\")\n",
    "        \n",
    "        return {\n",
    "            \"noise_model\": None,\n",
    "            \"fidelity_calculator\": None,\n",
    "            \"external_topology\": final_topology,\n",
    "            \"external_contexts\": contexts,\n",
    "            \"external_rewards\": external_rewards\n",
    "        }\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PAPER 2 (Huang et al.)\n",
    "    # ============================================================================\n",
    "    elif physics_model == \"paper2\":\n",
    "        p2_config = FRAMEWORK_CONFIG[\"paper2\"]\n",
    "        topo_gen = Paper2TopologyGenerator(num_nodes=p2_config[\"num_nodes\"], seed=base_seed)\n",
    "        topo = topo_gen.generate()\n",
    "        \n",
    "        try:\n",
    "            path_generator = nx.shortest_simple_paths(\n",
    "                topo, p2_config[\"source_node\"], p2_config[\"dest_node\"], weight=\"distance\"\n",
    "            )\n",
    "            paths = list(itertools.islice(path_generator, p2_config[\"num_paths\"]))\n",
    "        except nx.NetworkXNoPath:\n",
    "            paths = [[p2_config[\"source_node\"], p2_config[\"dest_node\"]]] * p2_config[\"num_paths\"]\n",
    "        \n",
    "        # Stochastic noise model\n",
    "        noise_model = FiberLossNoiseModel(\n",
    "            topology=topo,\n",
    "            paths=paths,\n",
    "            p_init=p2_config.get(\"p_init\", 0.00001),\n",
    "            f_attenuation=p2_config.get(\"f_attenuation\", 0.05)\n",
    "        )\n",
    "        \n",
    "        # Fidelity calculator with optional memory decay\n",
    "        if p2_config.get('use_memory_decay', False):\n",
    "            memory_model = MemoryNoiseModel(\n",
    "                T2=p2_config.get(\"memory_T2\", 5000),\n",
    "                swap_delay_per_link=p2_config.get(\"swap_delay_per_link\", 100)\n",
    "            )\n",
    "            if p2_config.get(\"swap_mode\", \"sync\") == \"sync\":\n",
    "                memory_model = None\n",
    "        else:\n",
    "            memory_model = None\n",
    "        \n",
    "        fidelity_calc = FullPaper2FidelityCalculator(\n",
    "            gate_error_rate=p2_config.get(\"gate_error_rate\", 0.02) if p2_config.get('use_gate_error', False) else 0.0,\n",
    "            memory_model=memory_model\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Paper2 Physics: Fiber Loss + {'Memory Decay' if memory_model else 'No Memory'}\")\n",
    "        \n",
    "        return {\n",
    "            \"noise_model\": noise_model,\n",
    "            \"fidelity_calculator\": fidelity_calc,\n",
    "            \"external_topology\": topo,\n",
    "            \"external_contexts\": None,\n",
    "            \"external_rewards\": None\n",
    "        }\n",
    "\n",
    "    # ============================================================================\n",
    "    # PAPER 12 (Wang et al. - QuARC)\n",
    "    # ============================================================================\n",
    "    elif physics_model == 'paper12':\n",
    "        p12config = FRAMEWORK_CONFIG['paper12']\n",
    "        \n",
    "        # Get base Paper12 physics\n",
    "        physics_params = get_physics_params_paper12(p12config, seed=base_seed, qubit_cap=qubit_cap)\n",
    "        base_fidelity_calc = physics_params['fidelity_calculator']\n",
    "        \n",
    "        # Wrap with retry logic\n",
    "        fidelity_calc = Paper12RetryFidelityCalculator(\n",
    "            base_calculator=base_fidelity_calc,\n",
    "            threshold=p12config['retry_threshold'],\n",
    "            max_attempts=p12config['max_retry_attempts'],\n",
    "            decay_rate=p12config['retry_decay_rate']\n",
    "        )\n",
    "        \n",
    "        physics_params['fidelity_calculator'] = fidelity_calc\n",
    "        \n",
    "        # Add metadata\n",
    "        metadata = {\n",
    "            'paper': 'Wang2024Paper12',\n",
    "            'retry_enabled': True,\n",
    "            'retry_threshold': p12config['retry_threshold'],\n",
    "            'max_attempts': p12config['max_retry_attempts'],\n",
    "            'decay_rate': p12config['retry_decay_rate'],\n",
    "        }\n",
    "        physics_params['metadata'] = metadata\n",
    "        \n",
    "        print(f\"üìä Paper12 Physics: Fusion (prob={p12config['fusion_prob']}) + Retry Logic\")\n",
    "        \n",
    "        return physics_params\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DEFAULT (No special physics)\n",
    "    # ============================================================================\n",
    "    else:\n",
    "        return {\n",
    "            \"noise_model\": None,\n",
    "            \"fidelity_calculator\": None,\n",
    "            \"external_topology\": topology,\n",
    "            \"external_contexts\": None,\n",
    "            \"external_rewards\": None\n",
    "        }\n",
    "\n",
    "\n",
    "def get_physics_params_paper12(config, seed, qubit_cap, num_paths = 4):\n",
    "    \"\"\"Paper #12 (Waxman + QuARC) physics adapter.\"\"\"\n",
    "    topology = Paper12WaxmanTopologyGenerator().generate()\n",
    "    num_paths = num_paths\n",
    "    nodes = list(topology.nodes())\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Find 4 paths\n",
    "    paths = []\n",
    "    attempts = 0\n",
    "    max_attempts = 10 * num_paths\n",
    "    \n",
    "    while len(paths) < num_paths and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        src, dst = rng.choice(nodes, 2, replace=False)\n",
    "        try:\n",
    "            path = nx.shortest_path(topology, src, dst)\n",
    "            if path not in paths:\n",
    "                paths.append(path)\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "    \n",
    "    if len(paths) < num_paths:\n",
    "        raise RuntimeError(f\"Could not find {num_paths} valid paths in Waxman topology\")\n",
    "\n",
    "    # Physics models\n",
    "    fusion_prob = float(config.get(\"fusion_prob\", 0.9))\n",
    "    entanglement_prob = float(config.get(\"entanglement_prob\", 0.6))\n",
    "    noise_model = FusionNoiseModel(\n",
    "        topology=topology, paths=paths, fusion_prob=fusion_prob, entanglement_prob=entanglement_prob\n",
    "    )\n",
    "    fidelity_calc = FusionFidelityCalculator()\n",
    "    reward_func = QuARCRewardFunction()\n",
    "\n",
    "    # Contexts: 4 arrays with shapes (8,3), (10,3), (8,3), (9,3)\n",
    "    external_contexts = []\n",
    "    arms_per_path = [8, 10, 8, 9]\n",
    "    degrees = dict(topology.degree())\n",
    "    max_degree = max(degrees.values()) if degrees else 1.0\n",
    "\n",
    "    for p_idx, K in enumerate(arms_per_path):\n",
    "        path = paths[p_idx]\n",
    "        hop_count = len(path) - 1\n",
    "        path_degrees = [degrees[n] for n in path]\n",
    "        avg_degree = float(sum(path_degrees) / len(path_degrees))\n",
    "        f2_deg_norm = avg_degree / max_degree if max_degree > 0 else 0.0\n",
    "        ctx = np.full((K, 3), [float(hop_count), f2_deg_norm, fusion_prob], dtype=float)\n",
    "        external_contexts.append(ctx)\n",
    "\n",
    "    # Rewards: 4 lists with lengths [8,10,8,9]\n",
    "    external_rewards = []\n",
    "    for p_idx, K in enumerate(arms_per_path):\n",
    "        path = paths[p_idx]\n",
    "        err_info = noise_model.get_error_rates(p_idx)\n",
    "        base_fidelity = fidelity_calc.compute_path_fidelity(err_info, context=None, fusion_prob=fusion_prob)\n",
    "        base_fidelity = float(np.clip(base_fidelity, 0.0, 1.0))\n",
    "\n",
    "        path_rewards = []\n",
    "        for _ in range(K):\n",
    "            success = rng.random() < base_fidelity\n",
    "            r = reward_func.compute_reward(success=success, aggregate_throughput=1)\n",
    "            path_rewards.append(float(r))\n",
    "        \n",
    "        external_rewards.append(path_rewards)\n",
    "    \n",
    "    return {\n",
    "        \"external_topology\": topology,\n",
    "        \"external_contexts\": external_contexts,\n",
    "        \"external_rewards\": external_rewards,\n",
    "        \"noise_model\": noise_model,\n",
    "        \"fidelity_calculator\": fidelity_calc,\n",
    "    }\n",
    "\n",
    "\n",
    "def force_release_resources(evaluator=None, verbose=True):\n",
    "    \"\"\"Force release of ALL resources that could block. Call AFTER each allocator completes.\"\"\"\n",
    "    cleanup_log = []\n",
    "\n",
    "    # 1. Stop logging and close file handles\n",
    "    if evaluator is not None:\n",
    "        try:\n",
    "            if hasattr(evaluator, 'configs') and hasattr(evaluator.configs, 'backup_mgr'):\n",
    "                backup_mgr = evaluator.configs.backup_mgr\n",
    "                if hasattr(backup_mgr, 'stop_logging_redirect'):\n",
    "                    backup_mgr.stop_logging_redirect()\n",
    "                if hasattr(backup_mgr, '_log_file'):\n",
    "                    try:\n",
    "                        backup_mgr._log_file.close()\n",
    "                    except:\n",
    "                        pass\n",
    "                if hasattr(backup_mgr, 'backup_registry'):\n",
    "                    backup_mgr.backup_registry.clear()\n",
    "            cleanup_log.append(\"‚úÖ Backup manager cleaned\")\n",
    "        except Exception as e:\n",
    "            cleanup_log.append(f\"‚ö†Ô∏è Backup cleanup: {e}\")\n",
    "\n",
    "    # 2. Clear environment graphs\n",
    "    if evaluator is not None:\n",
    "        try:\n",
    "            if hasattr(evaluator, 'configs') and hasattr(evaluator.configs, 'environment'):\n",
    "                env = evaluator.configs.environment\n",
    "                if hasattr(env, 'topology') and hasattr(env.topology, 'clear'):\n",
    "                    env.topology.clear()\n",
    "                    del env.topology\n",
    "                if hasattr(env, 'paths'):\n",
    "                    env.paths = []\n",
    "            cleanup_log.append(\"‚úÖ Environment graphs cleared\")\n",
    "        except Exception as e:\n",
    "            cleanup_log.append(f\"‚ö†Ô∏è Environment cleanup: {e}\")\n",
    "\n",
    "    # 3. Break circular references\n",
    "    if evaluator is not None:\n",
    "        try:\n",
    "            if hasattr(evaluator, 'configs'):\n",
    "                if hasattr(evaluator.configs, 'backup_mgr'):\n",
    "                    evaluator.configs.backup_mgr = None\n",
    "                if hasattr(evaluator.configs, 'environment'):\n",
    "                    evaluator.configs.environment = None\n",
    "                evaluator.configs = None\n",
    "            cleanup_log.append(\"‚úÖ Circular references broken\")\n",
    "        except Exception as e:\n",
    "            cleanup_log.append(f\"‚ö†Ô∏è Reference cleanup: {e}\")\n",
    "\n",
    "    # 4. Clear model registries\n",
    "    try:\n",
    "        import sys\n",
    "        for mod_name in list(sys.modules.keys()):\n",
    "            if 'bandit' in mod_name.lower() or 'neural' in mod_name.lower():\n",
    "                mod = sys.modules[mod_name]\n",
    "                if hasattr(mod, '_model_registry'):\n",
    "                    mod._model_registry.clear()\n",
    "                if hasattr(mod, '_global_models'):\n",
    "                    mod._global_models.clear()\n",
    "        cleanup_log.append(\"‚úÖ Model registries cleared\")\n",
    "    except Exception as e:\n",
    "        cleanup_log.append(f\"‚ö†Ô∏è Registry cleanup: {e}\")\n",
    "\n",
    "    # 5. Torch cleanup\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        cleanup_log.append(\"‚úÖ Torch CUDA cleared\")\n",
    "    except Exception as e:\n",
    "        cleanup_log.append(f\"‚ö†Ô∏è Torch cleanup: {e}\")\n",
    "\n",
    "    # 6. Garbage collection\n",
    "    collected = [gc.collect() for _ in range(3)]\n",
    "    cleanup_log.append(f\"‚úÖ GC collected: {sum(collected)} objects\")\n",
    "\n",
    "    # 7. Close file descriptors\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        for f in process.open_files():\n",
    "            if any(ext in f.path for ext in ['.pkl', '.log', '.csv']):\n",
    "                try:\n",
    "                    os.close(f.fd)\n",
    "                except:\n",
    "                    pass\n",
    "        cleanup_log.append(\"‚úÖ File descriptors closed\")\n",
    "    except Exception as e:\n",
    "        cleanup_log.append(f\"‚ö†Ô∏è FD cleanup: {e}\")\n",
    "\n",
    "    # 8. Delete evaluator and final collection\n",
    "    if evaluator is not None:\n",
    "        del evaluator\n",
    "    gc.collect(2)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üßπ FORCED RESOURCE RELEASE\")\n",
    "        print(\"=\"*70)\n",
    "        for log in cleanup_log:\n",
    "            print(log)\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION: Quick Paper 7 Sanity Check\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç PAPER 7 QUICK VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Test Paper 7 physics generation\n",
    "    test_params = get_physics_params(\n",
    "        physics_model='paper7',\n",
    "        base_seed=42\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Topology: {len(test_params['external_topology'].nodes())} nodes, \"\n",
    "          f\"{len(test_params['external_topology'].edges())} edges\")\n",
    "    print(f\"‚úÖ Contexts: {len(test_params['external_contexts'])} paths\")\n",
    "    print(f\"‚úÖ Rewards: {'Enabled' if test_params['external_rewards'] else 'Disabled'}\")\n",
    "    print(\"\\n‚úì Paper 7 integration validated successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"üöÄ Ready to run Paper 7 (QBGP) experiments!\")\n",
    "print(\"   Example: PHYSICS_MODELS = ['paper7']\")\n",
    "print(\"            ALLOCATORS = ['ThompsonSampling', 'DynamicUCB']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFYING NEURAL BANDIT FIXES ARE LOADED\n",
      "======================================================================\n",
      "‚úÖ NeuralUCB CLAMPING FIX detected\n",
      "‚úÖ EXPNeuralUCB PROBABILITY FIX detected\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verification: Check that fixes are in place\n",
    "print(\"=\" * 70)\n",
    "print(\"VERIFYING NEURAL BANDIT FIXES ARE LOADED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check NeuralUCB clamping fix\n",
    "from daqr.algorithms.base_bandit import NeuralUCB\n",
    "import inspect\n",
    "source = inspect.getsource(NeuralUCB.take_action)\n",
    "if \"np.maximum(p, 0.0)\" in source:\n",
    "    print(\"‚úÖ NeuralUCB CLAMPING FIX detected\")\n",
    "else:\n",
    "    print(\"‚ùå NeuralUCB CLAMPING FIX NOT found - reload failed!\")\n",
    "\n",
    "# Check EXPNeuralUCB probability fix  \n",
    "from daqr.algorithms.neural_bandits import EXPNeuralUCB\n",
    "source = inspect.getsource(EXPNeuralUCB._calculate_group_probabilities)\n",
    "if \"log_sum_exp\" in source or \"max_exponent\" in source:\n",
    "    print(\"‚úÖ EXPNeuralUCB PROBABILITY FIX detected\")\n",
    "else:\n",
    "    print(\"‚ùå EXPNeuralUCB PROBABILITY FIX NOT found - reload failed!\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DETAILED DEBUG: Enhanced error tracing\n",
      "======================================================================\n",
      "\n",
      "üóëÔ∏è CLEARING CACHED MODEL FILES...\n",
      "\n",
      "‚úÖ Enhanced debugging enabled\n"
     ]
    }
   ],
   "source": [
    "# Debug cell: Detailed traceback for GNeuralUCB error\n",
    "print(\"=\" * 70)\n",
    "print(\"DETAILED DEBUG: Enhanced error tracing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Patch numpy.random.choice to catch bad probability arrays\n",
    "original_choice = np.random.choice\n",
    "\n",
    "def debug_choice(a, size=None, replace=True, p=None, **kwargs):\n",
    "    if p is not None:\n",
    "        p_arr = np.asarray(p)\n",
    "        if np.any(p_arr < 0):\n",
    "            print(f\"\\n  [DEBUG-CHOICE] ‚ùå CAUGHT BAD PROBABILITIES!\")\n",
    "            print(f\"  [DEBUG-CHOICE]    p values: {p_arr}\")\n",
    "            print(f\"  [DEBUG-CHOICE]    min: {np.min(p_arr)}, max: {np.max(p_arr)}\")\n",
    "            print(f\"  [DEBUG-CHOICE]    sum: {np.sum(p_arr)}\")\n",
    "            print(f\"  [DEBUG-CHOICE]    Stack trace:\")\n",
    "            traceback.print_stack()\n",
    "            raise ValueError(\"probabilities are not non-negative\")\n",
    "    return original_choice(a, size=size, replace=replace, p=p, **kwargs)\n",
    "\n",
    "np.random.choice = debug_choice\n",
    "\n",
    "# Clear cache\n",
    "print(\"\\nüóëÔ∏è CLEARING CACHED MODEL FILES...\")\n",
    "cache_dirs = ['model_state', 'framework_state', 'quantum_logs', '.cache', '__pycache__']\n",
    "for cache_dir in cache_dirs:\n",
    "    try:\n",
    "        if os.path.exists(cache_dir):\n",
    "            shutil.rmtree(cache_dir)\n",
    "            print(f\"   ‚úÖ Cleared: {cache_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not clear {cache_dir}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced debugging enabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üóëÔ∏è CLEARING ALL CACHED MODEL FILES\n",
      "======================================================================\n",
      "‚ÑπÔ∏è  Not found: daqr/config/model_state\n",
      "‚ÑπÔ∏è  Not found: daqr/config/framework_state\n",
      "‚ÑπÔ∏è  Not found: daqr/config/quantum_logs\n",
      "\n",
      "‚úÖ Cache fully cleared - models will be fresh from fixed code\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CRITICAL: Clear All Cached Models Before Running Test\n",
    "# ============================================================\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üóëÔ∏è CLEARING ALL CACHED MODEL FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Specific cache directories used by the framework\n",
    "cache_paths = [\n",
    "    'daqr/config/model_state',\n",
    "    'daqr/config/framework_state',\n",
    "    'daqr/config/quantum_logs'\n",
    "]\n",
    "\n",
    "for cache_path in cache_paths:\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            shutil.rmtree(cache_path)\n",
    "            print(f\"‚úÖ Cleared: {cache_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error clearing {cache_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  Not found: {cache_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cache fully cleared - models will be fresh from fixed code\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PAPER #7 (QBGP) QUANTUM ROUTING EVALUATION - SINGLE ALLOCATOR TEST\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üéØ PAPER 7 ALLOCATOR EVALUATION\n",
      "======================================================================\n",
      "Allocator:                  Default\n",
      "Physics Model:              paper7\n",
      "Attack Scenarios:           ['stochastic']\n",
      "Scales:                     [1]\n",
      "Runs per Scale:             [1]\n",
      "Total Frames:               10\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ RUNNING: Default on Paper 7 (QBGP)\n",
      "======================================================================\n",
      "\n",
      "===================== GET LOCAL REGISTRY =====================\n",
      "‚Üí Skipping local cache (force=True or file missing)\n",
      "\n",
      "===================== BUILD LOCAL REGISTRY =====================\n",
      "\n",
      "================================================================================\n",
      "üîç SCANNING LOCAL FILES\n",
      "================================================================================\n",
      "Parameters: load_to_drive=False, force=False\n",
      "\n",
      "üìÇ Checking DRIVE mode: /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake\n",
      "   ‚ö†Ô∏è  Path does not exist, skipping\n",
      "\n",
      "üìÇ Checking LOCAL mode: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config\n",
      "   ‚úÖ Path exists, starting walk...\n",
      "   üö∂ Walking directory tree: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config\n",
      "\n",
      "   üìÅ Directory: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state/day_20260130\n",
      "      Relative: model_state/day_20260130\n",
      "      Component: model_state\n",
      "      Date: day_20260130\n",
      "      üìÑ Processing 19 files in model_state/day_20260130\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "      üìä model_state/day_20260130: 19/19 files processed\n",
      "      üìä model_state/day_20260130: 0/19 files skipped\n",
      "      üìä model_state/day_20260130: 0/19 files conflicted\n",
      "\n",
      "   üìÅ Directory: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state/day_20260130\n",
      "      Relative: framework_state/day_20260130\n",
      "      Component: framework_state\n",
      "      Date: day_20260130\n",
      "      üìÑ Processing 2 files in framework_state/day_20260130\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "      üìä framework_state/day_20260130: 2/2 files processed\n",
      "      üìä framework_state/day_20260130: 0/2 files skipped\n",
      "      üìä framework_state/day_20260130: 0/2 files conflicted\n",
      "\n",
      "   üìä Processed 2 directories\n",
      "\n",
      "================================================================================\n",
      "üì¶ BUILDING FINAL REGISTRY\n",
      "================================================================================\n",
      "Components found: ['model_state', 'framework_state']\n",
      "  ‚Ä¢ model_state: 19 files\n",
      "  ‚Ä¢ framework_state: 2 files\n",
      "\n",
      "‚úÖ SCAN COMPLETE\n",
      "Modes scanned: 1\n",
      "Total files in registry: 21\n",
      "================================================================================\n",
      "\n",
      "\t‚Üí Filesystem scan found 21 files\n",
      "\n",
      "===================== GET DRIVE REGISTRY =====================\n",
      "===================== REGISTRY BUILD COMPLETE =====================\n",
      "\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "\tüì¶ LocalBackupManager Registry Saved Locally in LOCAL\n",
      "BUILDING REGISTRY WITH 0 EXPECTED COMPONENTS KEYS\n",
      "\n",
      "===================== GET LOCAL REGISTRY =====================\n",
      "‚Üí Skipping local cache (force=True or file missing)\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "\tüì¶ LocalBackupManager Registry Saved Locally in LOCAL\n",
      "\n",
      "======================================================================\n",
      "üéØ AllocatorRunner initialized: Default\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ STARTING ALLOCATOR: Default\n",
      "======================================================================\n",
      "\n",
      "üìä Physics Model: paper7\n",
      "\n",
      "üìã Allocator Config:\n",
      "   Type: Default\n",
      "   Testbed: default\n",
      "   Paths: 15\n",
      "   Total Qubits: 75\n",
      "   Min per route: 2\n",
      "   Seed: 42\n",
      "‚úÖ Allocator created: (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "‚úì Allocator: QubitAllocator (15 paths)\n",
      "   Initial allocation: (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üìä Paper7 Topology: Real AS (as20000101.txt)\n",
      "üìä Paper7 Paths: 15 paths from 5-shortest between 3 ISPs\n",
      "üìä Paper7 Contexts: 15 context vectors generated\n",
      "üìä Paper7 Rewards: Context-aware (mode=neg_hop)\n",
      "‚è±Ô∏è  get_physics_params_paper7() time: 4.0 ms\n",
      "\n",
      "======================================================================\n",
      "RUN 1 | Scale: 1 | Exp: 1\n",
      "======================================================================\n",
      "üîÑ Default Dynamic Allocation (Initial): (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "Multi-Run Evaluator Initialized\n",
      "Environment Type: None\n",
      "Frame Range: 10 -> 10 (step: 10)\n",
      "quantum_exps-Default(paper7)_alloc-all_envs-5_attacks-10_10-1_runs-S1T\n",
      "[Logging Redirect Initialized]\n",
      "Log File: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/quantum_logs/quantum_exps-Default(paper7)_alloc-all_envs-5_attacks-10_10-1_runs-S1T_20260130_log.txt\n",
      "‚öô Running evaluation...\n",
      "======================================================================\n",
      "SCENARIOS MODEL EVALUATION\n",
      "======================================================================\n",
      "Testing algorithm performance against:\n",
      "\t‚Ä¢ Stochastic:  \tNatural quantum decoherence and network failures\n",
      "\t‚Ä¢ Baseline:    \tOptimal conditions for validation\n",
      "======================================================================\n",
      "Models to Test:             \tOracle, GNeuralUCB, EXPNeuralUCB, CPursuitNeuralUCB, iCPursuitNeuralUCB\n",
      "Experiments per Scenario:   \t1\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "TESTING ENVIRONMENT SCENARIO: BASELINE (OPTIMAL CONDITIONS)\n",
      "==================================================\n",
      "\n",
      "STARTING EXPERIMENTS: NONE\n",
      "Category: Baseline (No Attacks)\n",
      "============================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 1: 10 frames  <>  SCALED-CAPACITY: 10 frames (CAPACITY:10 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[QubitAllocator] timestep=0 ‚Üí allocation=(5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîß Allocated qubits for NONE Exp 1: (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîÑ Default Dynamic Allocation (Initial): (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "======================================================================================================================================================\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 1: Env:Baseline (None), Attack:No, Rate:0.0, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 14961\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 1: Starting GNeuralUCB           in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 GNEURALUCB          : Reward=0082.00, Efficiency=091.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.582]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 1: Starting EXPNeuralUCB         in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 EXPNEURALUCB        : Reward=0075.00, Efficiency=083.3% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.628]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 1: Starting CPursuitNeuralUCB    in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 CPURSUITNEURALUCB   : Reward=0082.00, Efficiency=091.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.634]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 1: Starting iCPursuitNeuralUCB   in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 ICPURSUITNEURALUCB  : Reward=0090.00, Efficiency=100.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.712]\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 1: Env:Baseline (None), Attack:No, Rate:0.0, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 14961\n",
      "\t-->üèÜ EXP1 Winner:iCPursuitNeuralUCB  (Gap:000.0%) [Env:Baseline (None), Attack:No X Rate:0.0, Frames:10, SCapacity=10, Alloc=Default]\n",
      "‚úì Experiment 1 completed successfully.\n",
      "Total experiment time: 144.0s\n",
      "Experiments completed for none\n",
      "\n",
      "\n",
      "\n",
      "TESTING ENVIRONMENT SCENARIO: STOCHASTIC RANDOM FAILURES\n",
      "==================================================\n",
      "\n",
      "STARTING EXPERIMENTS: STOCHASTIC\n",
      "Category: Stochastic (Natural Random Failures)\n",
      "============================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 1: 10 frames  <>  SCALED-CAPACITY: 10 frames (CAPACITY:10 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[QubitAllocator] timestep=0 ‚Üí allocation=(5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîß Allocated qubits for STOCHASTIC Exp 1: (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîÑ Default Dynamic Allocation (Initial): (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "======================================================================================================================================================\n",
      "\n",
      "STOCHASTIC (RANDOM) EXP 1: Env:Stochastic, Attack:Random, Rate:0.0625, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 16668\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ STOCHASTIC (RANDOM) EXP 1: Starting GNeuralUCB           in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 GNEURALUCB          : Reward=0082.00, Efficiency=091.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.582]\n",
      "\n",
      "\tüîÑ STOCHASTIC (RANDOM) EXP 1: Starting EXPNeuralUCB         in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 EXPNEURALUCB        : Reward=0073.00, Efficiency=081.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.628]\n",
      "\n",
      "\tüîÑ STOCHASTIC (RANDOM) EXP 1: Starting CPursuitNeuralUCB    in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 CPURSUITNEURALUCB   : Reward=0082.00, Efficiency=091.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.634]\n",
      "\n",
      "\tüîÑ STOCHASTIC (RANDOM) EXP 1: Starting iCPursuitNeuralUCB   in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 ICPURSUITNEURALUCB  : Reward=0081.00, Efficiency=090.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.712]\n",
      "\n",
      "STOCHASTIC (RANDOM) EXP 1: Env:Stochastic, Attack:Random, Rate:0.0625, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 16668\n",
      "\t-->üèÜ EXP1 Winner:GNeuralUCB          (Gap:008.9%) [Env:Stochastic, Attack:Random X Rate:0.0625, Frames:10, SCapacity=10, Alloc=Default]\n",
      "‚úì Experiment 1 completed successfully.\n",
      "Total experiment time: 144.5s\n",
      "Experiments completed for stochastic\n",
      "\n",
      "\n",
      "\n",
      "TESTING ENVIRONMENT SCENARIO: MARKOV ADVERSARIAL ATTACK\n",
      "==================================================\n",
      "\n",
      "STARTING EXPERIMENTS: MARKOV\n",
      "Category: Structured (Markov Chain Based)\n",
      "============================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 1: 10 frames  <>  SCALED-CAPACITY: 10 frames (CAPACITY:10 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[QubitAllocator] timestep=0 ‚Üí allocation=(5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîß Allocated qubits for MARKOV Exp 1: (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîÑ Default Dynamic Allocation (Initial): (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "======================================================================================================================================================\n",
      "\n",
      "ADVERSARIAL (MARKOV) EXP 1: Env:Adversarial, Attack:Markov, Rate:0.25, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 15254\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ ADVERSARIAL (MARKOV) EXP 1: Starting GNeuralUCB           in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 GNEURALUCB          : Reward=0057.00, Efficiency=064.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.582]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (MARKOV) EXP 1: Starting EXPNeuralUCB         in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 EXPNEURALUCB        : Reward=0045.00, Efficiency=050.6% [Retries=3, Failed=0, < Threshold=3, SCapacity=10, Threshold=0.628]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (MARKOV) EXP 1: Starting CPursuitNeuralUCB    in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 CPURSUITNEURALUCB   : Reward=0057.00, Efficiency=064.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.634]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (MARKOV) EXP 1: Starting iCPursuitNeuralUCB   in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 ICPURSUITNEURALUCB  : Reward=0018.00, Efficiency=020.2% [Retries=3, Failed=0, < Threshold=3, SCapacity=10, Threshold=0.712]\n",
      "\n",
      "ADVERSARIAL (MARKOV) EXP 1: Env:Adversarial, Attack:Markov, Rate:0.25, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 15254\n",
      "\t-->üèÜ EXP1 Winner:GNeuralUCB          (Gap:036.0%) [Env:Adversarial, Attack:Markov X Rate:0.25, Frames:10, SCapacity=10, Alloc=Default]\n",
      "‚úì Experiment 1 completed successfully.\n",
      "Total experiment time: 143.5s\n",
      "Experiments completed for markov\n",
      "\n",
      "\n",
      "\n",
      "TESTING ENVIRONMENT SCENARIO: ADAPTIVE ADVERSARIAL ATTACK\n",
      "==================================================\n",
      "\n",
      "STARTING EXPERIMENTS: ADAPTIVE\n",
      "Category: Adaptive (Reactive Strategic)\n",
      "============================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 1: 10 frames  <>  SCALED-CAPACITY: 10 frames (CAPACITY:10 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[QubitAllocator] timestep=0 ‚Üí allocation=(5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîß Allocated qubits for ADAPTIVE Exp 1: (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîÑ Default Dynamic Allocation (Initial): (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "======================================================================================================================================================\n",
      "\n",
      "ADVERSARIAL (ADAPTIVE) EXP 1: Env:Adversarial, Attack:Adaptive, Rate:0.25, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 17897\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ ADVERSARIAL (ADAPTIVE) EXP 1: Starting GNeuralUCB           in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 GNEURALUCB          : Reward=0067.00, Efficiency=074.4% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.582]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (ADAPTIVE) EXP 1: Starting EXPNeuralUCB         in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 EXPNEURALUCB        : Reward=0072.00, Efficiency=080.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.628]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (ADAPTIVE) EXP 1: Starting CPursuitNeuralUCB    in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 CPURSUITNEURALUCB   : Reward=0067.00, Efficiency=074.4% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.634]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (ADAPTIVE) EXP 1: Starting iCPursuitNeuralUCB   in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 ICPURSUITNEURALUCB  : Reward=0081.00, Efficiency=090.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.712]\n",
      "\n",
      "ADVERSARIAL (ADAPTIVE) EXP 1: Env:Adversarial, Attack:Adaptive, Rate:0.25, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 17897\n",
      "\t-->üèÜ EXP1 Winner:iCPursuitNeuralUCB  (Gap:010.0%) [Env:Adversarial, Attack:Adaptive X Rate:0.25, Frames:10, SCapacity=10, Alloc=Default]\n",
      "‚úì Experiment 1 completed successfully.\n",
      "Total experiment time: 144.7s\n",
      "Experiments completed for adaptive\n",
      "\n",
      "\n",
      "\n",
      "TESTING ENVIRONMENT SCENARIO: ONLINE ADAPTIVE ATTACK\n",
      "==================================================\n",
      "\n",
      "STARTING EXPERIMENTS: ONLINEADAPTIVE\n",
      "Category: Online Adaptive (Real-time Strategic)\n",
      "============================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 1: 10 frames  <>  SCALED-CAPACITY: 10 frames (CAPACITY:10 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[QubitAllocator] timestep=0 ‚Üí allocation=(5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîß Allocated qubits for ONLINEADAPTIVE Exp 1: (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "üîÑ Default Dynamic Allocation (Initial): (5, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 4)\n",
      "======================================================================================================================================================\n",
      "\n",
      "ADVERSARIAL (ONLINEADAPTIVE) EXP 1: Env:Adversarial, Attack:OnlineAdaptive, Rate:0.25, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 19949\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ ADVERSARIAL (ONLINEADAPTIVE) EXP 1: Starting GNeuralUCB           in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 GNEURALUCB          : Reward=0074.00, Efficiency=083.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.582]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (ONLINEADAPTIVE) EXP 1: Starting EXPNeuralUCB         in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 EXPNEURALUCB        : Reward=0041.00, Efficiency=046.1% [Retries=3, Failed=0, < Threshold=3, SCapacity=10, Threshold=0.628]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (ONLINEADAPTIVE) EXP 1: Starting CPursuitNeuralUCB    in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 CPURSUITNEURALUCB   : Reward=0074.00, Efficiency=083.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.634]\n",
      "\n",
      "\tüîÑ ADVERSARIAL (ONLINEADAPTIVE) EXP 1: Starting iCPursuitNeuralUCB   in sequence...\n",
      "\t‚úì Oracle model loaded from configs: Oracle\n",
      "\tEXP 1 ICPURSUITNEURALUCB  : Reward=0081.00, Efficiency=091.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.712]\n",
      "\n",
      "ADVERSARIAL (ONLINEADAPTIVE) EXP 1: Env:Adversarial, Attack:OnlineAdaptive, Rate:0.25, Frames:10, QubitAlloc=Default, SC:10 (Scale=1 x Cap=10), Seed: 19949\n",
      "\t-->üèÜ EXP1 Winner:iCPursuitNeuralUCB  (Gap:009.0%) [Env:Adversarial, Attack:OnlineAdaptive X Rate:0.25, Frames:10, SCapacity=10, Alloc=Default]\n",
      "‚úì Experiment 1 completed successfully.\n",
      "Total experiment time: 143.7s\n",
      "Experiments completed for onlineadaptive\n",
      "‚úì Scenario 'none' evaluation completed.\n",
      "‚úì Scenario 'stochastic' evaluation completed.\n",
      "‚úì Scenario 'markov' evaluation completed.\n",
      "‚úì Scenario 'adaptive' evaluation completed.\n",
      "‚úì Scenario 'onlineadaptive' evaluation completed.\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE SCENARIO PERFORMANCE ANALYSIS\n",
      "======================================================================\n",
      "SCENARIO: NONE\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: iCPursuitNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0090.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0000.00%\n",
      "\t‚Ä¢ Winner Avg Reward: 0090.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0100.00%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 0 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 1 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: NONE\n",
      "======================================================================\n",
      "SCENARIO: \tBASELINE (OPTIMAL CONDITIONS)\n",
      "----------------------------------------\n",
      "Recommended Model: \tiCPursuitNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t000.0%\n",
      "\tWinner Avg Efficiency: \t100.0%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t100.0% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ GNeuralUCB     : \t091.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t091.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t083.3% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "SCENARIO: STOCHASTIC\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: GNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0090.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0008.89%\n",
      "\t‚Ä¢ Winner Avg Reward: 0082.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0091.11%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 1 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 0 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: STOCHASTIC\n",
      "======================================================================\n",
      "SCENARIO: \tSTOCHASTIC RANDOM FAILURES\n",
      "----------------------------------------\n",
      "Recommended Model: \tGNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t008.9%\n",
      "\tWinner Avg Efficiency: \t091.1%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ GNeuralUCB     : \t091.1% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t091.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t090.0% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t081.1% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "SCENARIO: MARKOV\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: GNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0089.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0035.96%\n",
      "\t‚Ä¢ Winner Avg Reward: 0057.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0064.04%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 1 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 0 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: MARKOV\n",
      "======================================================================\n",
      "SCENARIO: \tMARKOV ADVERSARIAL ATTACK\n",
      "----------------------------------------\n",
      "Recommended Model: \tGNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t036.0%\n",
      "\tWinner Avg Efficiency: \t064.0%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ GNeuralUCB     : \t064.0% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t064.0% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t050.6% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t020.2% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "SCENARIO: ADAPTIVE\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: iCPursuitNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0090.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0010.00%\n",
      "\t‚Ä¢ Winner Avg Reward: 0081.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0090.00%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 0 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 1 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: ADAPTIVE\n",
      "======================================================================\n",
      "SCENARIO: \tADAPTIVE ADVERSARIAL ATTACK\n",
      "----------------------------------------\n",
      "Recommended Model: \tiCPursuitNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t010.0%\n",
      "\tWinner Avg Efficiency: \t090.0%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t090.0% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t080.0% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ GNeuralUCB     : \t074.4% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t074.4% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "SCENARIO: ONLINEADAPTIVE\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: iCPursuitNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0089.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0008.99%\n",
      "\t‚Ä¢ Winner Avg Reward: 0081.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0091.01%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 0 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 1 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: ONLINEADAPTIVE\n",
      "======================================================================\n",
      "SCENARIO: \tONLINE ADAPTIVE ATTACK\n",
      "----------------------------------------\n",
      "Recommended Model: \tiCPursuitNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t009.0%\n",
      "\tWinner Avg Efficiency: \t091.0%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t091.0% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ GNeuralUCB     : \t083.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t083.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t046.1% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "KEY INSIGHTS:\n",
      "Best Performing Model Analysis (GNeuralUCB):\n",
      "\t‚Ä¢ Stochastic Performance:    \t82.000\n",
      "\t‚Ä¢ Baseline Performance:      \t90.000\n",
      "\t‚Ä¢ Performance Retention:     \t091.1%\n",
      "\tGOOD:               \tAcceptable performance under stochastic conditions\n",
      "Statistical Analysis:\n",
      "\t‚Ä¢ Total models evaluated:     \t5\n",
      "\t‚Ä¢ Experiments per environment:\t1\n",
      "\t‚Ä¢ Quantum network simulation: \tComprehensive stochastic modeling\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE SCENARIO PERFORMANCE ANALYSIS\n",
      "======================================================================\n",
      "SCENARIO: NONE\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: iCPursuitNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0090.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0000.00%\n",
      "\t‚Ä¢ Winner Avg Reward: 0090.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0100.00%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 0 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 1 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: NONE\n",
      "======================================================================\n",
      "SCENARIO: \tBASELINE (OPTIMAL CONDITIONS)\n",
      "----------------------------------------\n",
      "Recommended Model: \tiCPursuitNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t000.0%\n",
      "\tWinner Avg Efficiency: \t100.0%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t100.0% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ GNeuralUCB     : \t091.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t091.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t083.3% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "SCENARIO: STOCHASTIC\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: GNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0090.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0008.89%\n",
      "\t‚Ä¢ Winner Avg Reward: 0082.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0091.11%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 1 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 0 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: STOCHASTIC\n",
      "======================================================================\n",
      "SCENARIO: \tSTOCHASTIC RANDOM FAILURES\n",
      "----------------------------------------\n",
      "Recommended Model: \tGNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t008.9%\n",
      "\tWinner Avg Efficiency: \t091.1%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ GNeuralUCB     : \t091.1% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t091.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t090.0% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t081.1% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "SCENARIO: MARKOV\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: GNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0089.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0035.96%\n",
      "\t‚Ä¢ Winner Avg Reward: 0057.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0064.04%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 1 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 0 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: MARKOV\n",
      "======================================================================\n",
      "SCENARIO: \tMARKOV ADVERSARIAL ATTACK\n",
      "----------------------------------------\n",
      "Recommended Model: \tGNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t036.0%\n",
      "\tWinner Avg Efficiency: \t064.0%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ GNeuralUCB     : \t064.0% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t064.0% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t050.6% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t020.2% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "SCENARIO: ADAPTIVE\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: iCPursuitNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0090.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0010.00%\n",
      "\t‚Ä¢ Winner Avg Reward: 0081.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0090.00%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 0 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 1 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: ADAPTIVE\n",
      "======================================================================\n",
      "SCENARIO: \tADAPTIVE ADVERSARIAL ATTACK\n",
      "----------------------------------------\n",
      "Recommended Model: \tiCPursuitNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t010.0%\n",
      "\tWinner Avg Efficiency: \t090.0%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t090.0% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t080.0% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ GNeuralUCB     : \t074.4% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t074.4% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "SCENARIO: ONLINEADAPTIVE\n",
      "\t‚Ä¢ Total Experiments: 1\n",
      "\t‚Ä¢ Overall Winner: iCPursuitNeuralUCB\n",
      "\t‚Ä¢ Oracle Avg Reward: 0089.00\n",
      "\t‚Ä¢ Winner Avg Gap: 0008.99%\n",
      "\t‚Ä¢ Winner Avg Reward: 0081.00\n",
      "\t‚Ä¢ Winner Avg Efficiency: 0091.01%\n",
      "\t‚Ä¢ Win Counts:\n",
      "\t\t- Oracle: 0 wins\n",
      "\t\t- GNeuralUCB: 0 wins\n",
      "\t\t- EXPNeuralUCB: 0 wins\n",
      "\t\t- CPursuitNeuralUCB: 0 wins\n",
      "\t\t- iCPursuitNeuralUCB: 1 wins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DETAILED SCENARIO PERFORMANCE: ONLINEADAPTIVE\n",
      "======================================================================\n",
      "SCENARIO: \tONLINE ADAPTIVE ATTACK\n",
      "----------------------------------------\n",
      "Recommended Model: \tiCPursuitNeuralUCB (Won 1/1 experiments)\n",
      "\tWinner Avg Gap: \t009.0%\n",
      "\tWinner Avg Efficiency: \t091.0%\n",
      "\n",
      "Overall Models Performance:\n",
      "\t‚Ä¢ iCPursuitNeuralUCB: \t091.0% Efficiency \t(Won 1/1 experiments)\n",
      "\t‚Ä¢ GNeuralUCB     : \t083.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ CPursuitNeuralUCB: \t083.1% Efficiency \t(Won 0/1 experiments)\n",
      "\t‚Ä¢ EXPNeuralUCB   : \t046.1% Efficiency \t(Won 0/1 experiments)\n",
      "======================================================================\n",
      "‚úÖ Evaluation completed!\n",
      "[Logging Redirect Stopped]\n",
      "\n",
      "üßπ Final cleanup for Default\n",
      "\n",
      "======================================================================\n",
      "‚úÖ COMPLETED: Default\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Default COMPLETED SUCCESSFULLY\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ALLOCATOR TEST COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Allocator + ExperimentConfiguration for Paper #7\n",
    "# ============================================================\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.quantum_physics              import *\n",
    "from daqr.evaluation.allocator_runner       import AllocatorRunner\n",
    "from daqr.evaluation.multi_run_evaluator    import MultiRunEvaluator\n",
    "from daqr.config.experiment_config          import ExperimentConfiguration\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAPER #7 (QBGP) QUANTUM ROUTING EVALUATION - SINGLE ALLOCATOR TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Single Allocator Selection\n",
    "# ------------------------------------------------------------\n",
    "allocator_type = \"Default\"  # Options: \"Random\", \"DynamicUCB\", \"ThompsonSampling\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Run Parameters\n",
    "# ------------------------------------------------------------\n",
    "attack_intensity    = FRAMEWORK_CONFIG['intensity']\n",
    "current_frames      = 10\n",
    "frame_step          = 10\n",
    "current_experiments = 1\n",
    "last_backup         = False\n",
    "base_cap            = False\n",
    "overwrite           = True  # CRITICAL: Force fresh models, don't load old cached ones\n",
    "\n",
    "FRAMEWORK_CONFIG['exp_num']        = current_experiments\n",
    "FRAMEWORK_CONFIG['base_frames']    = current_frames\n",
    "FRAMEWORK_CONFIG['frame_step']     = frame_step\n",
    "\n",
    "    # 'exp_num': 5,\n",
    "    # 'test_mode': True,\n",
    "    # 'base_frames': 4000,\n",
    "    # 'frame_step': 2000,\n",
    "    # 'models': models,\n",
    "    # 'intensity': 0.25,\n",
    "    # 'routing_strategy': 'fixed',\n",
    "    # 'capacity': 10000,\n",
    "    # 'main_env': 'stochastic',\n",
    "\n",
    "\n",
    "# Testbed Configuration\n",
    "PHYSICS_MODELS = ['paper7']  # Paper 7 (QBGP)\n",
    "ATTACK_SCENARIOS = ['stochastic']  # Start simple\n",
    "SCALES = [1]\n",
    "RUNS = [1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ PAPER 7 ALLOCATOR EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Allocator:                  {allocator_type}\")\n",
    "print(f\"Physics Model:              {PHYSICS_MODELS[0]}\")\n",
    "print(f\"Attack Scenarios:           {ATTACK_SCENARIOS}\")\n",
    "print(f\"Scales:                     {SCALES}\")\n",
    "print(f\"Runs per Scale:             {RUNS}\")\n",
    "print(f\"Total Frames:               {current_frames}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run allocator\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ RUNNING: {allocator_type} on Paper 7 (QBGP)\")\n",
    "print('='*70)\n",
    "\n",
    "try:\n",
    "    # Create isolated runner instance\n",
    "    custom_config = ExperimentConfiguration(\n",
    "        env_type=FRAMEWORK_CONFIG['main_env'],\n",
    "        scenarios=test_scenarios,\n",
    "        use_last_backup=last_backup,\n",
    "        models=models,\n",
    "        attack_intensity=attack_intensity,\n",
    "        scale=1,\n",
    "        base_capacity=base_cap,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "\n",
    "    alloc_runner = AllocatorRunner(\n",
    "        allocator_type=allocator_type,\n",
    "        physics_models=PHYSICS_MODELS,\n",
    "        framework_config=FRAMEWORK_CONFIG,\n",
    "        scales=SCALES,\n",
    "        runs=RUNS,\n",
    "        models=models,\n",
    "        test_scenarios=test_scenarios,\n",
    "        config=custom_config\n",
    "    )\n",
    "\n",
    "    # Run with Paper 7 physics\n",
    "    alloc_runner.run(get_physics_params_func=get_physics_params)\n",
    "    print(f\"\\n‚úÖ {allocator_type} COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå {allocator_type} FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALLOCATOR TEST COMPLETE!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reloading fixed neural bandit modules...\n",
      "PyTorch version: 2.8.0+cu128\n",
      "NumPy version: 1.26.4\n",
      "Using device: cpu\n",
      "‚úÖ Neural bandit modules reloaded with probability fixes\n",
      "   - NeuralUCB.take_action() now clamps negative values\n",
      "   - EXPNeuralUCB._calculate_group_probabilities() is now robust\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RELOAD FIXED MODULES (Neural Network Probability Fix)\n",
    "# ============================================================\n",
    "print(\"üîÑ Reloading fixed neural bandit modules...\")\n",
    "\n",
    "# Reload the fixed modules to apply the probability fixes\n",
    "importlib.reload(base_bandit)\n",
    "importlib.reload(neural_bandits)\n",
    "\n",
    "print(\"‚úÖ Neural bandit modules reloaded with probability fixes\")\n",
    "print(\"   - NeuralUCB.take_action() now clamps negative values\")\n",
    "print(\"   - EXPNeuralUCB._calculate_group_probabilities() is now robust\")\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "robustness-analysis",
    "outputId": "d5bf77ec-ed26-474f-b68b-650ab683fece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PAPER #7 (QBGP) QUANTUM ROUTING EVALUATION - SINGLE ALLOCATOR TEST\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üéØ PAPER 7 ALLOCATOR EVALUATION\n",
      "======================================================================\n",
      "Allocator:                  Dynamic\n",
      "Physics Model:              paper7\n",
      "Attack Scenarios:           ['stochastic']\n",
      "Scales:                     [1]\n",
      "Runs per Scale:             [5]\n",
      "Total Frames:               50\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ RUNNING: Dynamic on Paper 7 (QBGP)\n",
      "======================================================================\n",
      "\n",
      "===================== GET LOCAL REGISTRY =====================\n",
      "‚Üí Skipping local cache (force=True or file missing)\n",
      "\n",
      "===================== BUILD LOCAL REGISTRY =====================\n",
      "\n",
      "================================================================================\n",
      "üîç SCANNING LOCAL FILES\n",
      "================================================================================\n",
      "Parameters: load_to_drive=False, force=False\n",
      "\n",
      "üìÇ Checking DRIVE mode: /content/drive/Shareddrives/ai_quantum_computing/quantum_data_lake\n",
      "   ‚ö†Ô∏è  Path does not exist, skipping\n",
      "\n",
      "üìÇ Checking LOCAL mode: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config\n",
      "   ‚úÖ Path exists, starting walk...\n",
      "   üö∂ Walking directory tree: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config\n",
      "\n",
      "   üìÅ Directory: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/model_state/day_20260130\n",
      "      Relative: model_state/day_20260130\n",
      "      Component: model_state\n",
      "      Date: day_20260130\n",
      "      üìÑ Processing 37 files in model_state/day_20260130\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "      üìä model_state/day_20260130: 37/37 files processed\n",
      "      üìä model_state/day_20260130: 0/37 files skipped\n",
      "      üìä model_state/day_20260130: 0/37 files conflicted\n",
      "\n",
      "   üìÅ Directory: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/framework_state/day_20260130\n",
      "      Relative: framework_state/day_20260130\n",
      "      Component: framework_state\n",
      "      Date: day_20260130\n",
      "      üìÑ Processing 6 files in framework_state/day_20260130\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "      üìä framework_state/day_20260130: 6/6 files processed\n",
      "      üìä framework_state/day_20260130: 0/6 files skipped\n",
      "      üìä framework_state/day_20260130: 0/6 files conflicted\n",
      "\n",
      "   üìä Processed 2 directories\n",
      "\n",
      "================================================================================\n",
      "üì¶ BUILDING FINAL REGISTRY\n",
      "================================================================================\n",
      "Components found: ['model_state', 'framework_state']\n",
      "  ‚Ä¢ model_state: 37 files\n",
      "  ‚Ä¢ framework_state: 6 files\n",
      "\n",
      "‚úÖ SCAN COMPLETE\n",
      "Modes scanned: 1\n",
      "Total files in registry: 43\n",
      "================================================================================\n",
      "\n",
      "\t‚Üí Filesystem scan found 43 files\n",
      "\n",
      "===================== GET DRIVE REGISTRY =====================\n",
      "===================== REGISTRY BUILD COMPLETE =====================\n",
      "\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "\tüì¶ LocalBackupManager Registry Saved Locally in LOCAL\n",
      "BUILDING REGISTRY WITH 0 EXPECTED COMPONENTS KEYS\n",
      "\n",
      "===================== GET LOCAL REGISTRY =====================\n",
      "‚Üí Skipping local cache (force=True or file missing)\n",
      "\n",
      "==================== FETCH FROM DRIVE START ====================\n",
      "‚ö†Ô∏è Drive NOT available -> cannot fetch registry\n",
      "\tüì¶ LocalBackupManager Registry Saved Locally in LOCAL\n",
      "\n",
      "======================================================================\n",
      "üéØ AllocatorRunner initialized: Dynamic\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ STARTING ALLOCATOR: Dynamic\n",
      "======================================================================\n",
      "\n",
      "üìä Physics Model: paper7\n",
      "\n",
      "üìã Allocator Config:\n",
      "   Type: Dynamic\n",
      "   Testbed: default\n",
      "   Paths: 15\n",
      "   Total Qubits: 75\n",
      "   Min per route: 2\n",
      "   Seed: 42\n",
      "   Exploration bonus: 2.0\n",
      "‚úÖ Allocator created: (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "‚úì Allocator: DynamicQubitAllocator (15 paths)\n",
      "   Initial allocation: (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üìä Paper7 Topology: Real AS (as20000101.txt)\n",
      "üìä Paper7 Paths: 15 paths from 5-shortest between 3 ISPs\n",
      "üìä Paper7 Contexts: 15 context vectors generated\n",
      "üìä Paper7 Rewards: Context-aware (mode=neg_hop)\n",
      "‚è±Ô∏è  get_physics_params_paper7() time: 4.2 ms\n",
      "\n",
      "======================================================================\n",
      "RUN 1 | Scale: 1 | Exp: 5\n",
      "======================================================================\n",
      "üîÑ Dynamic Dynamic Allocation (Initial): (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "Multi-Run Evaluator Initialized\n",
      "Environment Type: None\n",
      "Frame Range: 10 -> 50 (step: 10)\n",
      "quantum_exps-Dynamic(paper7)_alloc-all_envs-5_attacks-10_10-5_runs-S1T\n",
      "[Logging Redirect Initialized]\n",
      "Log File: /workspaces/quantum_project/Dynamic_Routing_Eval_Framework/daqr/config/quantum_logs/quantum_exps-Dynamic(paper7)_alloc-all_envs-5_attacks-10_10-5_runs-S1T_20260130_log.txt\n",
      "‚öô Running evaluation...\n",
      "======================================================================\n",
      "SCENARIOS MODEL EVALUATION\n",
      "======================================================================\n",
      "Testing algorithm performance against:\n",
      "\t‚Ä¢ Stochastic:  \tNatural quantum decoherence and network failures\n",
      "\t‚Ä¢ Baseline:    \tOptimal conditions for validation\n",
      "======================================================================\n",
      "Models to Test:             \tOracle, GNeuralUCB, EXPNeuralUCB, CPursuitNeuralUCB, iCPursuitNeuralUCB\n",
      "Experiments per Scenario:   \t5\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "TESTING ENVIRONMENT SCENARIO: BASELINE (OPTIMAL CONDITIONS)\n",
      "==================================================\n",
      "\n",
      "STARTING EXPERIMENTS: NONE\n",
      "Category: Baseline (No Attacks)\n",
      "============================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 1: 10 frames  <>  SCALED-CAPACITY: 10 frames (CAPACITY:10 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DynamicAllocator] timestep=0 (uniform init) ‚Üí allocation=(5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üîß Allocated qubits for NONE Exp 1: (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üîÑ Dynamic Dynamic Allocation (Initial): (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "======================================================================================================================================================\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 1: Env:Baseline (None), Attack:No, Rate:0.0, Frames:10, QubitAlloc=Dynamic, SC:10 (Scale=1 x Cap=10), Seed: 14961\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 1: Starting GNeuralUCB           in sequence...\n",
      "\tEXP 1 GNEURALUCB          : Reward=0082.00, Efficiency=091.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.582]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 1: Starting EXPNeuralUCB         in sequence...\n",
      "\tEXP 1 EXPNEURALUCB        : Reward=0075.00, Efficiency=083.3% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.628]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 1: Starting CPursuitNeuralUCB    in sequence...\n",
      "\tEXP 1 CPURSUITNEURALUCB   : Reward=0082.00, Efficiency=091.1% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.634]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 1: Starting iCPursuitNeuralUCB   in sequence...\n",
      "\tEXP 1 ICPURSUITNEURALUCB  : Reward=0090.00, Efficiency=100.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=10, Threshold=0.712]\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 1: Env:Baseline (None), Attack:No, Rate:0.0, Frames:10, QubitAlloc=Dynamic, SC:10 (Scale=1 x Cap=10), Seed: 14961\n",
      "\t-->üèÜ EXP1 Winner:iCPursuitNeuralUCB  (Gap:000.0%) [Env:Baseline (None), Attack:No X Rate:0.0, Frames:10, SCapacity=10, Alloc=Dynamic]\n",
      "‚úì Experiment 1 completed successfully.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 2: 20 frames  <>  SCALED-CAPACITY: 20 frames (CAPACITY:20 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DynamicAllocator] timestep=1 (uniform init) ‚Üí allocation=(5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üîß Allocated qubits for NONE Exp 2: (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üîÑ Dynamic Dynamic Allocation (Initial): (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "======================================================================================================================================================\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 2: Env:Baseline (None), Attack:No, Rate:0.0, Frames:20, QubitAlloc=Dynamic, SC:20 (Scale=1 x Cap=20), Seed: 16300\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 2: Starting GNeuralUCB           in sequence...\n",
      "\tEXP 2 GNEURALUCB          : Reward=0160.00, Efficiency=088.9% [Retries=0, Failed=0, < Threshold=0, SCapacity=20, Threshold=0.582]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 2: Starting EXPNeuralUCB         in sequence...\n",
      "\tEXP 2 EXPNEURALUCB        : Reward=0158.00, Efficiency=087.8% [Retries=0, Failed=0, < Threshold=0, SCapacity=20, Threshold=0.628]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 2: Starting CPursuitNeuralUCB    in sequence...\n",
      "\tEXP 2 CPURSUITNEURALUCB   : Reward=0160.00, Efficiency=088.9% [Retries=0, Failed=0, < Threshold=0, SCapacity=20, Threshold=0.634]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 2: Starting iCPursuitNeuralUCB   in sequence...\n",
      "\tEXP 2 ICPURSUITNEURALUCB  : Reward=0180.00, Efficiency=100.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=20, Threshold=0.712]\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 2: Env:Baseline (None), Attack:No, Rate:0.0, Frames:20, QubitAlloc=Dynamic, SC:20 (Scale=1 x Cap=20), Seed: 16300\n",
      "\t-->üèÜ EXP2 Winner:iCPursuitNeuralUCB  (Gap:000.0%) [Env:Baseline (None), Attack:No X Rate:0.0, Frames:20, SCapacity=20, Alloc=Dynamic]\n",
      "‚úì Experiment 2 completed successfully.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 3: 30 frames  <>  SCALED-CAPACITY: 30 frames (CAPACITY:30 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DynamicAllocator] timestep=2 (uniform init) ‚Üí allocation=(5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üîß Allocated qubits for NONE Exp 3: (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üîÑ Dynamic Dynamic Allocation (Initial): (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "======================================================================================================================================================\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 3: Env:Baseline (None), Attack:No, Rate:0.0, Frames:30, QubitAlloc=Dynamic, SC:30 (Scale=1 x Cap=30), Seed: 13275\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 3: Starting GNeuralUCB           in sequence...\n",
      "\tEXP 3 GNEURALUCB          : Reward=0240.00, Efficiency=088.9% [Retries=0, Failed=0, < Threshold=0, SCapacity=30, Threshold=0.582]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 3: Starting EXPNeuralUCB         in sequence...\n",
      "\tEXP 3 EXPNEURALUCB        : Reward=0237.00, Efficiency=087.8% [Retries=0, Failed=0, < Threshold=0, SCapacity=30, Threshold=0.628]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 3: Starting CPursuitNeuralUCB    in sequence...\n",
      "\tEXP 3 CPURSUITNEURALUCB   : Reward=0240.00, Efficiency=088.9% [Retries=0, Failed=0, < Threshold=0, SCapacity=30, Threshold=0.634]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 3: Starting iCPursuitNeuralUCB   in sequence...\n",
      "\tEXP 3 ICPURSUITNEURALUCB  : Reward=0270.00, Efficiency=100.0% [Retries=0, Failed=0, < Threshold=0, SCapacity=30, Threshold=0.712]\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 3: Env:Baseline (None), Attack:No, Rate:0.0, Frames:30, QubitAlloc=Dynamic, SC:30 (Scale=1 x Cap=30), Seed: 13275\n",
      "\t-->üèÜ EXP3 Winner:iCPursuitNeuralUCB  (Gap:000.0%) [Env:Baseline (None), Attack:No X Rate:0.0, Frames:30, SCapacity=30, Alloc=Dynamic]\n",
      "‚úì Experiment 3 completed successfully.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "EXPERIMENT 4: 40 frames  <>  SCALED-CAPACITY: 40 frames (CAPACITY:40 X SCALE:1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DynamicAllocator] timestep=3 (uniform init) ‚Üí allocation=(5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üîß Allocated qubits for NONE Exp 4: (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "üîÑ Dynamic Dynamic Allocation (Initial): (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "======================================================================================================================================================\n",
      "\n",
      "BASELINE (NONE) (NO) EXP 4: Env:Baseline (None), Attack:No, Rate:0.0, Frames:40, QubitAlloc=Dynamic, SC:40 (Scale=1 x Cap=40), Seed: 17796\n",
      "======================================================================================================================================================\n",
      "\tGetting Oracle Rewards ...\n",
      "\tüìä Reward structure for Oracle:\n",
      "\t   Type: <class 'list'>\n",
      "\t   Length: 15\n",
      "\t   First reward: [9.0], type: <class 'list'>\n",
      "\t   First reward length: 1\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 4: Starting GNeuralUCB           in sequence...\n",
      "\tEXP 4 GNEURALUCB          : Reward=0318.00, Efficiency=088.3% [Retries=0, Failed=0, < Threshold=0, SCapacity=40, Threshold=0.582]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 4: Starting EXPNeuralUCB         in sequence...\n",
      "\tEXP 4 EXPNEURALUCB        : Reward=0314.00, Efficiency=087.2% [Retries=0, Failed=0, < Threshold=0, SCapacity=40, Threshold=0.628]\n",
      "\n",
      "\tüîÑ BASELINE (NONE) (NO) EXP 4: Starting CPursuitNeuralUCB    in sequence...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Allocator + ExperimentConfiguration for Paper #7\n",
    "# ============================================================\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.quantum_physics              import *\n",
    "from daqr.evaluation.allocator_runner       import AllocatorRunner\n",
    "from daqr.evaluation.multi_run_evaluator    import MultiRunEvaluator\n",
    "from daqr.config.experiment_config          import ExperimentConfiguration\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAPER #7 (QBGP) QUANTUM ROUTING EVALUATION - SINGLE ALLOCATOR TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Single Allocator Selection\n",
    "# ------------------------------------------------------------\n",
    "allocator_type = \"Dynamic\"  # Options: \"Random\", \"DynamicUCB\", \"ThompsonSampling\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Run Parameters\n",
    "# ------------------------------------------------------------\n",
    "attack_intensity    = FRAMEWORK_CONFIG['intensity']\n",
    "current_frames      = 50\n",
    "frame_step          = 50\n",
    "current_experiments = 1\n",
    "last_backup         = False\n",
    "base_cap            = False\n",
    "overwrite           = True\n",
    "\n",
    "# Testbed Configuration\n",
    "PHYSICS_MODELS = ['paper7']  # Paper 7 (QBGP)\n",
    "ATTACK_SCENARIOS = ['stochastic']  # Start simple\n",
    "SCALES = [1]\n",
    "RUNS = [5]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ PAPER 7 ALLOCATOR EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Allocator:                  {allocator_type}\")\n",
    "print(f\"Physics Model:              {PHYSICS_MODELS[0]}\")\n",
    "print(f\"Attack Scenarios:           {ATTACK_SCENARIOS}\")\n",
    "print(f\"Scales:                     {SCALES}\")\n",
    "print(f\"Runs per Scale:             {RUNS}\")\n",
    "print(f\"Total Frames:               {current_frames}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run allocator\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ RUNNING: {allocator_type} on Paper 7 (QBGP)\")\n",
    "print('='*70)\n",
    "\n",
    "try:\n",
    "    # Create isolated runner instance\n",
    "    custom_config = ExperimentConfiguration(\n",
    "        env_type=FRAMEWORK_CONFIG['main_env'],\n",
    "        scenarios=test_scenarios,\n",
    "        use_last_backup=last_backup,\n",
    "        models=models,\n",
    "        attack_intensity=attack_intensity,\n",
    "        scale=1,\n",
    "        base_capacity=base_cap,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "\n",
    "    alloc_runner = AllocatorRunner(\n",
    "        allocator_type=allocator_type,\n",
    "        physics_models=PHYSICS_MODELS,\n",
    "        framework_config=FRAMEWORK_CONFIG,\n",
    "        scales=SCALES,\n",
    "        runs=RUNS,\n",
    "        models=models,\n",
    "        test_scenarios=test_scenarios,\n",
    "        config=custom_config\n",
    "    )\n",
    "\n",
    "    # Run with Paper 7 physics\n",
    "    alloc_runner.run(get_physics_params_func=get_physics_params)\n",
    "    print(f\"\\n‚úÖ {allocator_type} COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå {allocator_type} FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALLOCATOR TEST COMPLETE!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Allocator + ExperimentConfiguration for Paper #2\n",
    "# ============================================================\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.quantum_physics              import *\n",
    "from daqr.core.qubit_allocator              import (\n",
    "                                                QubitAllocator,\n",
    "                                                RandomQubitAllocator,\n",
    "                                                DynamicQubitAllocator,\n",
    "                                                ThompsonSamplingAllocator\n",
    "                                            )\n",
    "\n",
    "from daqr.evaluation.allocator_runner       import AllocatorRunner\n",
    "from daqr.evaluation.multi_run_evaluator    import MultiRunEvaluator\n",
    "from daqr.config.experiment_config          import ExperimentConfiguration\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 70, \"\\nPAPER #2 QUANTUM MAB MODELS EVALUATION FRAMEWORK - STOCHASTIC FOCUS\\n\", \"=\" * 70)\n",
    "# ------------------------------------------------------------\n",
    "# 1) Allocator selection (Paper #2: start with fixed baseline)\n",
    "# ------------------------------------------------------------\n",
    "# You can swap these as needed:\n",
    "# allocator = RandomQubitAllocator(epsilon=0.5, seed=42) # ============================================================\n",
    "# Cell 2: Allocator + ExperimentConfiguration for Paper #7\n",
    "# ============================================================\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.quantum_physics              import *\n",
    "from daqr.evaluation.allocator_runner       import AllocatorRunner\n",
    "from daqr.evaluation.multi_run_evaluator    import MultiRunEvaluator\n",
    "from daqr.config.experiment_config          import ExperimentConfiguration\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAPER #7 (QBGP) QUANTUM ROUTING EVALUATION - SINGLE ALLOCATOR TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Single Allocator Selection\n",
    "# ------------------------------------------------------------\n",
    "allocator_type = \"ThompsonSampling\"  # Options: \"Random\", \"DynamicUCB\", \"ThompsonSampling\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Run Parameters\n",
    "# ------------------------------------------------------------\n",
    "attack_intensity    = FRAMEWORK_CONFIG['intensity']\n",
    "current_frames      = 50\n",
    "frame_step          = 50\n",
    "current_experiments = 1\n",
    "last_backup         = False\n",
    "base_cap            = False\n",
    "overwrite           = True\n",
    "\n",
    "# Testbed Configuration\n",
    "PHYSICS_MODELS = ['paper7']  # Paper 7 (QBGP)\n",
    "ATTACK_SCENARIOS = ['stochastic']  # Start simple\n",
    "SCALES = [1]\n",
    "RUNS = [5]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ PAPER 7 ALLOCATOR EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Allocator:                  {allocator_type}\")\n",
    "print(f\"Physics Model:              {PHYSICS_MODELS[0]}\")\n",
    "print(f\"Attack Scenarios:           {ATTACK_SCENARIOS}\")\n",
    "print(f\"Scales:                     {SCALES}\")\n",
    "print(f\"Runs per Scale:             {RUNS}\")\n",
    "print(f\"Total Frames:               {current_frames}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run allocator\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ RUNNING: {allocator_type} on Paper 7 (QBGP)\")\n",
    "print('='*70)\n",
    "\n",
    "try:\n",
    "    # Create isolated runner instance\n",
    "    custom_config = ExperimentConfiguration(\n",
    "        env_type=FRAMEWORK_CONFIG['main_env'],\n",
    "        scenarios=test_scenarios,\n",
    "        use_last_backup=last_backup,\n",
    "        models=models,\n",
    "        attack_intensity=attack_intensity,\n",
    "        scale=1,\n",
    "        base_capacity=base_cap,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "\n",
    "    alloc_runner = AllocatorRunner(\n",
    "        allocator_type=allocator_type,\n",
    "        physics_models=PHYSICS_MODELS,\n",
    "        framework_config=FRAMEWORK_CONFIG,\n",
    "        scales=SCALES,\n",
    "        runs=RUNS,\n",
    "        models=models,\n",
    "        test_scenarios=test_scenarios,\n",
    "        config=custom_config\n",
    "    )\n",
    "\n",
    "    # Run with Paper 7 physics\n",
    "    alloc_runner.run(get_physics_params_func=get_physics_params)\n",
    "    print(f\"\\n‚úÖ {allocator_type} COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå {allocator_type} FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALLOCATOR TEST COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "       # 50-50 mix\n",
    "# allocator = RandomQubitAllocator(epsilon=0.1, seed=42)        # mostly baseline\n",
    "# allocator = RandomQubitAllocator(epsilon=0.0, seed=42)        # deterministic\n",
    "# allocator = DynamicQubitAllocator(seed=42)                    # dynamic UCB\n",
    "# allocator = ThompsonSamplingAllocator(seed=42)                # Thompson\n",
    "\n",
    "allocator_type =    \"ThompsonSampling\" # Fixed allocator via environment (Paper #2 baseline)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Derive run parameters from FRAMEWORK_CONFIG (Cell 1)\n",
    "# ------------------------------------------------------------\n",
    "# current_frames      = FRAMEWORK_CONFIG['base_frames'] \n",
    "# frame_step          = FRAMEWORK_CONFIG['frame_step']\n",
    "# attack_intensity    = FRAMEWORK_CONFIG['intensity']\n",
    "# current_experiments = FRAMEWORK_CONFIG['exp_num']\n",
    "\n",
    "attack_intensity    = FRAMEWORK_CONFIG['intensity']\n",
    "current_frames      = 50\n",
    "frame_step          = 50\n",
    "current_experiments = 1\n",
    "last_backup         = False\n",
    "base_cap            = False\n",
    "overwrite           = True\n",
    "\n",
    "# PHYSICS_MODELS = ['paper2', 'default']  # Set to ['default', 'paper2'] to test both\n",
    "ATTACK_SCENARIOS = ['stochastic']  # Start simple, expand later\n",
    "PHYSICS_MODELS = ['paper2']\n",
    "# PHYSICS_MODELS = ['default']\n",
    "ALLOCATORS = ['Default', 'Dynamic', 'ThompsonSampling', 'Random']\n",
    "SCALES = [2]\n",
    "RUNS = [5]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70, \"\\nüéØ QUANTUM ROUTING ALLOCATOR EVALUATION\\n\", \"=\" * 70)\n",
    "print(f\"Total Allocators to Test:   {len(ALLOCATORS)}\")\n",
    "print(f\"Physics Models:             {PHYSICS_MODELS}\")\n",
    "print(f\"Allocators:                 {ALLOCATORS}\")\n",
    "print(f\"Scales:                     {SCALES}\")\n",
    "print(f\"Runs:                       {RUNS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run each allocator in isolation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ ALLOCATOR {len(ALLOCATORS)}: {allocator_type}\")\n",
    "print('='*70)\n",
    "\n",
    "try:\n",
    "    # Create isolated runner instance for this allocator\n",
    "    custom_config = ExperimentConfiguration(\n",
    "        env_type=FRAMEWORK_CONFIG['main_env'], scenarios=test_scenarios, use_last_backup=last_backup,\n",
    "        models=models, attack_intensity=attack_intensity, scale=2, base_capacity=base_cap, overwrite=overwrite)\n",
    "\n",
    "    alloc_runner = AllocatorRunner(\n",
    "        allocator_type=allocator_type, physics_models=PHYSICS_MODELS, framework_config=FRAMEWORK_CONFIG, \n",
    "        scales=SCALES, runs=RUNS, models=models, test_scenarios=test_scenarios, config=custom_config)\n",
    "\n",
    "    # Run this allocator with your existing get_physics_params function\n",
    "    alloc_runner.run(get_physics_params_func=get_physics_params)\n",
    "    print(f\"\\n‚úÖ {allocator_type} COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå {allocator_type} FAILED: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL ALLOCATORS COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Allocator + ExperimentConfiguration for Paper #2\n",
    "# ============================================================\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.quantum_physics              import *\n",
    "from daqr.core.qubit_allocator              import (\n",
    "                                                QubitAllocator,\n",
    "                                                RandomQubitAllocator,\n",
    "                                                DynamicQubitAllocator,\n",
    "                                                ThompsonSamplingAllocator\n",
    "                                            )\n",
    "\n",
    "from daqr.evaluation.allocator_runner       import AllocatorRunner\n",
    "from daqr.evaluation.multi_run_evaluator    import MultiRunEvaluator\n",
    "from daqr.config.experiment_config          import ExperimentConfiguration\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 70, \"\\nPAPER #2 QUANTUM MAB MODELS EVALUATION FRAMEWORK - STOCHASTIC FOCUS\\n\", \"=\" * 70)\n",
    "# ------------------------------------------------------------\n",
    "# 1) Allocator selection (Paper #2: start with fixed baseline)\n",
    "# ------------------------------------------------------------\n",
    "# You can swap these as needed:\n",
    "# allocator = RandomQubitAllocator(epsilon=0.5, seed=42) # ============================================================\n",
    "# Cell 2: Allocator + ExperimentConfiguration for Paper #7\n",
    "# ============================================================\n",
    "importlib.reload(qubit_allocator)\n",
    "importlib.reload(experiment_config)\n",
    "importlib.reload(multi_run_evaluator)\n",
    "\n",
    "from daqr.core.quantum_physics              import *\n",
    "from daqr.evaluation.allocator_runner       import AllocatorRunner\n",
    "from daqr.evaluation.multi_run_evaluator    import MultiRunEvaluator\n",
    "from daqr.config.experiment_config          import ExperimentConfiguration\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAPER #7 (QBGP) QUANTUM ROUTING EVALUATION - SINGLE ALLOCATOR TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Single Allocator Selection\n",
    "# ------------------------------------------------------------\n",
    "allocator_type = \"Random\"  # Options: \"Random\", \"DynamicUCB\", \"ThompsonSampling\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Run Parameters\n",
    "# ------------------------------------------------------------\n",
    "attack_intensity    = FRAMEWORK_CONFIG['intensity']\n",
    "current_frames      = 50\n",
    "frame_step          = 50\n",
    "current_experiments = 1\n",
    "last_backup         = False\n",
    "base_cap            = False\n",
    "overwrite           = True\n",
    "\n",
    "# Testbed Configuration\n",
    "PHYSICS_MODELS = ['paper7']  # Paper 7 (QBGP)\n",
    "ATTACK_SCENARIOS = ['stochastic']  # Start simple\n",
    "SCALES = [1]\n",
    "RUNS = [5]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ PAPER 7 ALLOCATOR EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Allocator:                  {allocator_type}\")\n",
    "print(f\"Physics Model:              {PHYSICS_MODELS[0]}\")\n",
    "print(f\"Attack Scenarios:           {ATTACK_SCENARIOS}\")\n",
    "print(f\"Scales:                     {SCALES}\")\n",
    "print(f\"Runs per Scale:             {RUNS}\")\n",
    "print(f\"Total Frames:               {current_frames}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run allocator\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ RUNNING: {allocator_type} on Paper 7 (QBGP)\")\n",
    "print('='*70)\n",
    "\n",
    "try:\n",
    "    # Create isolated runner instance\n",
    "    custom_config = ExperimentConfiguration(\n",
    "        env_type=FRAMEWORK_CONFIG['main_env'],\n",
    "        scenarios=test_scenarios,\n",
    "        use_last_backup=last_backup,\n",
    "        models=models,\n",
    "        attack_intensity=attack_intensity,\n",
    "        scale=1,\n",
    "        base_capacity=base_cap,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "\n",
    "    alloc_runner = AllocatorRunner(\n",
    "        allocator_type=allocator_type,\n",
    "        physics_models=PHYSICS_MODELS,\n",
    "        framework_config=FRAMEWORK_CONFIG,\n",
    "        scales=SCALES,\n",
    "        runs=RUNS,\n",
    "        models=models,\n",
    "        test_scenarios=test_scenarios,\n",
    "        config=custom_config\n",
    "    )\n",
    "\n",
    "    # Run with Paper 7 physics\n",
    "    alloc_runner.run(get_physics_params_func=get_physics_params)\n",
    "    print(f\"\\n‚úÖ {allocator_type} COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå {allocator_type} FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALLOCATOR TEST COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "       # 50-50 mix\n",
    "# allocator = RandomQubitAllocator(epsilon=0.1, seed=42)        # mostly baseline\n",
    "# allocator = RandomQubitAllocator(epsilon=0.0, seed=42)        # deterministic\n",
    "# allocator = DynamicQubitAllocator(seed=42)                    # dynamic UCB\n",
    "# allocator = ThompsonSamplingAllocator(seed=42)                # Thompson\n",
    "\n",
    "allocator_type =    \"ThompsonSampling\" # Fixed allocator via environment (Paper #2 baseline)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Derive run parameters from FRAMEWORK_CONFIG (Cell 1)\n",
    "# ------------------------------------------------------------\n",
    "# current_frames      = FRAMEWORK_CONFIG['base_frames'] \n",
    "# frame_step          = FRAMEWORK_CONFIG['frame_step']\n",
    "# attack_intensity    = FRAMEWORK_CONFIG['intensity']\n",
    "# current_experiments = FRAMEWORK_CONFIG['exp_num']\n",
    "\n",
    "attack_intensity    = FRAMEWORK_CONFIG['intensity']\n",
    "current_frames      = 50\n",
    "frame_step          = 50\n",
    "current_experiments = 1\n",
    "last_backup         = False\n",
    "base_cap            = False\n",
    "overwrite           = True\n",
    "\n",
    "# PHYSICS_MODELS = ['paper2', 'default']  # Set to ['default', 'paper2'] to test both\n",
    "ATTACK_SCENARIOS = ['stochastic']  # Start simple, expand later\n",
    "PHYSICS_MODELS = ['paper2']\n",
    "# PHYSICS_MODELS = ['default']\n",
    "ALLOCATORS = ['Default', 'Dynamic', 'ThompsonSampling', 'Random']\n",
    "SCALES = [2]\n",
    "RUNS = [5]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70, \"\\nüéØ QUANTUM ROUTING ALLOCATOR EVALUATION\\n\", \"=\" * 70)\n",
    "print(f\"Total Allocators to Test:   {len(ALLOCATORS)}\")\n",
    "print(f\"Physics Models:             {PHYSICS_MODELS}\")\n",
    "print(f\"Allocators:                 {ALLOCATORS}\")\n",
    "print(f\"Scales:                     {SCALES}\")\n",
    "print(f\"Runs:                       {RUNS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run each allocator in isolation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ ALLOCATOR {len(ALLOCATORS)}: {allocator_type}\")\n",
    "print('='*70)\n",
    "\n",
    "try:\n",
    "    # Create isolated runner instance for this allocator\n",
    "    custom_config = ExperimentConfiguration(\n",
    "        env_type=FRAMEWORK_CONFIG['main_env'], scenarios=test_scenarios, use_last_backup=last_backup,\n",
    "        models=models, attack_intensity=attack_intensity, scale=2, base_capacity=base_cap, overwrite=overwrite)\n",
    "\n",
    "    alloc_runner = AllocatorRunner(\n",
    "        allocator_type=allocator_type, physics_models=PHYSICS_MODELS, framework_config=FRAMEWORK_CONFIG, \n",
    "        scales=SCALES, runs=RUNS, models=models, test_scenarios=test_scenarios, config=custom_config)\n",
    "\n",
    "    # Run this allocator with your existing get_physics_params function\n",
    "    alloc_runner.run(get_physics_params_func=get_physics_params)\n",
    "    print(f\"\\n‚úÖ {allocator_type} COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå {allocator_type} FAILED: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL ALLOCATORS COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comprehensive-evaluation"
   },
   "source": [
    "## Multi-Environment Performance Analysis\n",
    "\n",
    "### Complete Evaluation Matrix\n",
    "\n",
    "This research extends beyond the primary stochastic-adversarial comparison to provide comprehensive algorithm assessment across the complete spectrum of operational environments, establishing a thorough empirical foundation for robustness evaluation.\n",
    "\n",
    "### Environmental Test Framework\n",
    "\n",
    "| Environment | Classification | Threat Characteristics | Analytical Purpose |\n",
    "|-------------|---------------|----------------------|-------------------|\n",
    "| `none` | Baseline | Deterministic optimal conditions | Theoretical performance ceiling |\n",
    "| `stochastic` | Probabilistic | Uniform random failures | Standard operational baseline |\n",
    "| `markov` | Adversarial | Memory-dependent strategic attacks | Oblivious adversarial model |\n",
    "| `adaptive` | Adversarial | Feedback-driven strategic attacks | Responsive adversarial model |\n",
    "| `onlineadaptive` | Adversarial | Real-time adaptive strategic attacks | Sophisticated adversarial model |\n",
    "\n",
    "### Research Contributions\n",
    "\n",
    "**Comprehensive Threat Model Coverage**\n",
    "The evaluation framework addresses the complete spectrum of operational conditions, from optimal deterministic environments through increasingly sophisticated adversarial scenarios, providing unprecedented coverage of realistic deployment conditions.\n",
    "\n",
    "**Graduated Adversarial Complexity Analysis**  \n",
    "The systematic progression from oblivious to sophisticated adversarial models enables precise quantification of algorithm performance degradation as threat sophistication increases, revealing critical robustness thresholds.\n",
    "\n",
    "**Cross-Environment Validation Protocol**\n",
    "Consistent algorithm ranking across multiple environments validates robustness claims and identifies algorithms with stable performance characteristics independent of operational conditions.\n",
    "\n",
    "**Empirical Robustness Quantification**\n",
    "The multi-environment approach enables precise measurement of performance degradation rates, establishing quantitative robustness metrics that support theoretical predictions and practical deployment decisions.\n",
    "\n",
    "### Methodological Significance\n",
    "\n",
    "This comprehensive evaluation protocol addresses limitations in existing literature where algorithm assessment often focuses on narrow operational scenarios, providing the empirical foundation necessary for robust algorithm deployment in practical quantum network environments."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1rknAIThhNzWIoGwNHJR_N0F6hBb7T3e-",
     "timestamp": 1759053280580
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
